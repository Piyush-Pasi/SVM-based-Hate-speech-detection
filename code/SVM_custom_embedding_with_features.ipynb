{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eNED-J1-ZkBj"
      },
      "outputs": [],
      "source": [
        "!pip install chars2vec"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "obDyRFWmiS6D"
      },
      "outputs": [],
      "source": [
        "!pip install sentence-transformers\n",
        "from sentence_transformers import SentenceTransformer\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hbt_aRUsA_5G"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "from nltk.corpus import wordnet\n",
        "import pandas as pd\n",
        "import random\n",
        "import numpy as np\n",
        "import chars2vec\n",
        "import sklearn.decomposition\n",
        "import matplotlib.pyplot as plt\n",
        "import pickle\n",
        "from collections import defaultdict\n",
        "import string\n",
        "import re\n",
        "from gensim.models import KeyedVectors\n",
        "\n",
        "from sklearn.model_selection import train_test_split #split data into train and test sets\n",
        "from sklearn.feature_extraction.text import CountVectorizer #convert text comment into a numeric vector\n",
        "from sklearn.feature_extraction.text import TfidfTransformer, TfidfVectorizer #use TF IDF transformer to change text vector created by count vectorizer\n",
        "from sklearn.svm import SVC# Support Vector Machine\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.ensemble import BaggingClassifier, RandomForestClassifier, AdaBoostClassifier\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('punkt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iy5IVwhdJVg9",
        "outputId": "823fb143-bcea-49fe-8c89-4bf0b632041c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "# !pip install google.colab\n",
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MJPZ2rF_-7Ai"
      },
      "source": [
        "# Preprocessing Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "yrH7mqk8l7v7"
      },
      "outputs": [],
      "source": [
        "# !pip install chars2vec \n",
        "c2v_model = chars2vec.load_model('eng_300')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "vT-g_sZuA1CT"
      },
      "outputs": [],
      "source": [
        "def get_synonyms(word):\n",
        "    synonyms = []\n",
        "    for syn in wordnet.synsets(word):\n",
        "        for l in syn.lemmas():\n",
        "            synonyms.append(l.name())\n",
        "    return synonyms\n",
        "stop = nltk.corpus.stopwords.words('english')\n",
        "\n",
        "def augment_data(sent):\n",
        "    words = sent.split()\n",
        "    words = [w if w not in stop else '@'+w for w in words]\n",
        "    for i in range(len(words)):\n",
        "        if not words[i].startswith('@'):\n",
        "            syn_w = get_synonyms(words[i])\n",
        "            if syn_w != []:\n",
        "                w = random.choice(syn_w)\n",
        "                words[i] = \" \".join(w.split('_'))\n",
        "        else:\n",
        "            words[i] = words[i][1:]\n",
        "    return \" \".join(words)\n",
        "\n",
        "def preprocess_text(s):\n",
        "\n",
        "    s = s.replace('\\n',' ')\n",
        "    s = s.replace('\\t',' ')\n",
        "    s = s.replace('7','s')\n",
        "    s = s.replace('2','to')\n",
        "    s = s.replace('8','ight')\n",
        "    s = s.split()\n",
        "    s = [i for i in s if i]\n",
        "    s = \" \".join(s)\n",
        "    s = s.split()\n",
        "    return \" \".join(s)\n",
        "\n",
        "def transform_x(df):\n",
        "    x = df.apply(lambda row : preprocess_text(row['comment_text']), axis=1)\n",
        "    return pd.DataFrame(x,columns=['comment_text'])\n",
        "\n",
        "def merge(df1,df2):\n",
        "    return pd.concat([df1, df2], axis=1)\n",
        "\n",
        "def drop_faulty_rows(df):\n",
        "    return df.drop(df[(df['toxic'] == -1.0) & (df['severe_toxic'] == -1.0) & \n",
        "                    (df['obscene'] == -1.0) & (df['threat'] == -1.0) & \n",
        "                    (df['insult'] == -1.0) & (df['identity_hate'] == -1.0) ].index)\n",
        "    \n",
        "def combine_labels(train_df):\n",
        "    x = np.where(train_df['toxic']+train_df['severe_toxic']+train_df['obscene']\n",
        "             +train_df['threat']+train_df['insult']+train_df['identity_hate'] > 0, 1, 0)\n",
        "    return pd.DataFrame(x,columns=['Toxic'])\n",
        "\n",
        "#Adjust the path here accordingly\n",
        "w2v_whole_data = KeyedVectors.load_word2vec_format('/content/drive/MyDrive/NLP_Project_IITB/Project/jigsaw-toxic-comment-classification-challenge/data/custom_glove_768d.txt')\n",
        "n_dim = 768\n",
        "\n",
        "def get_word_vec(word):\n",
        "    try:\n",
        "         return w2v_whole_data.word_vec(word)\n",
        "    except:\n",
        "        return np.zeros(n_dim) \n",
        "vect_get_word_vec = np.vectorize(get_word_vec)\n",
        "\n",
        "\n",
        "def get_sentence_embed(sent):\n",
        "    words = np.array(sent.split())\n",
        "    if len(words)==0:\n",
        "        return np.zeros(n_dim)\n",
        "    word_vecs = np.array([vect_get_word_vec(x) for x in words])\n",
        "    return np.average(word_vecs,axis=0)\n",
        "\n",
        "\n",
        "def get_sentence_embed_tf_idf(sent):\n",
        "    global tf_idf_dict\n",
        "    words = np.array(sent.split())\n",
        "    if len(words) == 0:\n",
        "        return np.zeros(n_dim)\n",
        "    word_vecs = np.array([vect_get_word_vec(x) for x in words])\n",
        "    for i in range(len(words)):\n",
        "        word_vecs[i] = tf_idf_dict[words[i]]*word_vecs[i]\n",
        "    return np.average(word_vecs,axis=0)\n",
        "\n",
        "stop_words = set(nltk.corpus.stopwords.words('english'))  \n",
        "\n",
        "def get_sentence_embed_tf_idf_with_features(sent):\n",
        "  global tf_idf_dict\n",
        "  words = np.array(sent.split())\n",
        "  if len(words) == 0:\n",
        "      return np.zeros(n_dim)\n",
        "  word_vecs = np.array([vect_get_word_vec(x) for x in words])\n",
        "  for i in range(len(words)):\n",
        "      word_vecs[i] = tf_idf_dict[words[i]]*word_vecs[i]\n",
        "  vec =  np.average(word_vecs,axis=0).tolist()\n",
        "\n",
        "  #Total Length of sentences\n",
        "  vec.append(len(sent))\n",
        "\n",
        "  #Total number of words\n",
        "  vec.append(len(words))\n",
        "\n",
        "  #Number of captial characters\n",
        "  caps = 0\n",
        "  for ch in sent:\n",
        "    if ch.isupper():\n",
        "      caps += 1\n",
        "  vec.append(caps)\n",
        "\n",
        "  #Number of all capital words\n",
        "  caps = 0\n",
        "  for ch in words:\n",
        "    if ch.isupper():\n",
        "      caps += 1\n",
        "  vec.append(caps)\n",
        "  \n",
        "  #Number of exclamation marks \n",
        "  vec.append(sent.count('!'))\n",
        "  \n",
        "\n",
        "  #Number of question marks \n",
        "  vec.append(sent.count('?'))\n",
        "\n",
        "\n",
        "  #Number of punctuations marks \n",
        "  vec.append(sent.count('.')+sent.count(',')+sent.count(';')+sent.count(':')) \n",
        "\n",
        "  #Number of symbols *&$% marks \n",
        "  vec.append(sent.count('*')+sent.count('&')+sent.count('$')+sent.count('%')) \n",
        "\n",
        "  #Number of words inside quotes single or double:\n",
        "  x = re.findall('\"([^\"]*)\"', sent)\n",
        "  y = re.findall(\"'([^']*)'\", sent)\n",
        "  vec.append(len(x)+len(y))\n",
        "\n",
        "  #Number of sentences:\n",
        "  vec.append(len(nltk.sent_tokenize(sent)))\n",
        "\n",
        "  #Count the number of unique words\n",
        "  vec.append(len(set(words)))\n",
        "\n",
        "  #Count of hashtags\n",
        "  vec.append(len(re.findall(r'(#[A-Za-z0-9]*)', sent)))\n",
        "\n",
        "\n",
        "  #Count of mentions\n",
        "  vec.append(len(re.findall(r'(@[A-Za-z0-9]*)', sent)))\n",
        "\n",
        "\n",
        "  #Count of stopwords\n",
        "  vec.append(len([w for w in words if w in stop_words]))\n",
        "  \n",
        "\n",
        "  #Calculating average word length\n",
        "  vec.append(len(sent)/len(words))\n",
        "\n",
        "  #Calculating average sentence length\n",
        "  vec.append(len(words)/len(nltk.sent_tokenize(sent)))\n",
        "  \n",
        "\n",
        "  #unique words vs word count feature\n",
        "  vec.append(len(set(words))/len(words))\n",
        "  \n",
        "\n",
        "  #Stopwords count vs words counts feature\n",
        "  vec.append(len([w for w in words if w in stop_words])/len(words))\n",
        "  \n",
        "  return np.array(vec)\n",
        "\n",
        "def feature_normalise_param(X):\n",
        "  res = []\n",
        "  Y = X\n",
        "  #Total Length of sentences\n",
        "  res.append(X['comment_text'].apply(len).max())\n",
        "\n",
        "  #Total number of words\n",
        "  res.append(X['comment_text'].apply(lambda x: len(x.split())).max())\n",
        "\n",
        "  #Number of captial characters\n",
        "  res.append(X['comment_text'].apply(lambda x: len(re.findall(\"([A-Z])\",x))).max())\n",
        "  \n",
        "  #Number of all capital words\n",
        "  res.append(X['comment_text'].apply(lambda x: len([1 for y in x.split() if y.isupper()])).max())\n",
        "  \n",
        "  #Number of exclamation marks \n",
        "  res.append(X['comment_text'].apply(lambda x: x.count('!')).max())\n",
        "\n",
        "  #Number of question marks \n",
        "  res.append(X['comment_text'].apply(lambda x: x.count('?')).max())\n",
        "\n",
        "  #Number of punctuations marks \n",
        "  res.append(X['comment_text'].apply(lambda x: x.count('.') + x.count(',') + x.count(';') + x.count(':')).max())\n",
        "\n",
        "  #Number of symbols *&$% marks \n",
        "  res.append(X['comment_text'].apply(lambda x: x.count('*') + x.count('&') + x.count('$') + x.count('%')).max())\n",
        "\n",
        "  #Number of words inside quotes single or double:\n",
        "  res.append(X['comment_text'].apply(lambda x: len(re.findall('\"([^\"]*)\"', x)) + len(re.findall(\"'([^']*)'\", x))).max())\n",
        "\n",
        "  #Number of sentences:\n",
        "  res.append(X['comment_text'].apply(lambda x: len(nltk.sent_tokenize(x))).max())\n",
        "\n",
        "  #Count the number of unique words\n",
        "  res.append(X['comment_text'].apply(lambda x: len(set(x.split()))).max())\n",
        "  \n",
        "  #Count of hashtags\n",
        "  res.append(X['comment_text'].apply(lambda x: len(re.findall(r'(#[A-Za-z0-9]*)', x))).max())\n",
        "\n",
        "  #Count of mentions\n",
        "  res.append(X['comment_text'].apply(lambda x: len(re.findall(r'(@[A-Za-z0-9]*)', x))).max())\n",
        "\n",
        "  #Count of stopwords\n",
        "  res.append(X['comment_text'].apply(lambda x: len([w for w in x.split() if w in stop_words])).max())\n",
        "  \n",
        "  return res\n",
        "\n",
        "def get_sentence_embed_tf_idf_with_features_norm(sent,maxs):\n",
        "  global tf_idf_dict\n",
        "  words = np.array(sent.split())\n",
        "  if len(words) == 0:\n",
        "      return np.zeros(n_dim)\n",
        "  word_vecs = np.array([vect_get_word_vec(x) for x in words])\n",
        "  for i in range(len(words)):\n",
        "      word_vecs[i] = tf_idf_dict[words[i]]*word_vecs[i]\n",
        "  vec =  np.average(word_vecs,axis=0).tolist()\n",
        "\n",
        "\n",
        "  #Total Length of sentences\n",
        "  vec.append(len(sent)/maxs[0])\n",
        "\n",
        "  #Total number of words\n",
        "  vec.append(len(words)/maxs[1])\n",
        "\n",
        "  #Number of captial characters\n",
        "  caps = 0\n",
        "  for ch in sent:\n",
        "    if ch.isupper():\n",
        "      caps += 1\n",
        "  vec.append(caps/maxs[2])\n",
        "\n",
        "  #Number of all capital words\n",
        "  caps = 0\n",
        "  for ch in words:\n",
        "    if ch.isupper():\n",
        "      caps += 1\n",
        "  vec.append(caps/maxs[3])\n",
        "  \n",
        "  #Number of exclamation marks \n",
        "  vec.append(sent.count('!')/maxs[4])\n",
        "  \n",
        "  #Number of question marks \n",
        "  vec.append(sent.count('?')/maxs[5])\n",
        "\n",
        "  #Number of punctuations marks \n",
        "  vec.append((sent.count('.')+sent.count(',')+sent.count(';')+sent.count(':'))/maxs[6]) \n",
        "\n",
        "  #Number of symbols *&$% marks \n",
        "  vec.append((sent.count('*')+sent.count('&')+sent.count('$')+sent.count('%'))/maxs[7]) \n",
        "\n",
        "  #Number of words inside quotes single or double:\n",
        "  x = re.findall('\"([^\"]*)\"', sent)\n",
        "  y = re.findall(\"'([^']*)'\", sent)\n",
        "  vec.append((len(x)+len(y))/maxs[8])\n",
        "\n",
        "  #Number of sentences:\n",
        "  vec.append(len(nltk.sent_tokenize(sent))/maxs[9])\n",
        "\n",
        "  #Count the number of unique words\n",
        "  vec.append(len(set(words))/maxs[10])\n",
        "\n",
        "  #Count of hashtags\n",
        "  vec.append(len(re.findall(r'(#[A-Za-z0-9]*)', sent))/maxs[11])\n",
        "\n",
        "  #Count of mentions\n",
        "  vec.append(len(re.findall(r'(@[A-Za-z0-9]*)', sent))/maxs[12])\n",
        "\n",
        "  #Count of stopwords\n",
        "  vec.append(len([w for w in words if w in stop_words])/maxs[13])\n",
        "  \n",
        "  #Calculating average word length\n",
        "  vec.append(len(sent)/len(words))\n",
        "\n",
        "  #Calculating average sentence length\n",
        "  vec.append(len(words)/len(nltk.sent_tokenize(sent)))\n",
        "  \n",
        "\n",
        "  #unique words vs word count feature\n",
        "  vec.append(len(set(words))/len(words))\n",
        "  \n",
        "\n",
        "  #Stopwords count vs words counts feature\n",
        "  vec.append(len([w for w in words if w in stop_words])/len(words))\n",
        "  \n",
        "  return np.array(vec)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yt2mcrtU-2fV"
      },
      "source": [
        "# Training Data Transformation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "7WxGM85MaiBy",
        "outputId": "5093bcc8-0c49-48bb-c3e9-1b04e170ab75"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>comment_text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Explanation Why the edits made under my userna...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>D'aww! He matches this background colour I'm s...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Hey man, I'm really not trying to edit war. It...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>\" More I can't make any real suggestions on im...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>You, sir, are my hero. Any chance you remember...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                        comment_text\n",
              "0  Explanation Why the edits made under my userna...\n",
              "1  D'aww! He matches this background colour I'm s...\n",
              "2  Hey man, I'm really not trying to edit war. It...\n",
              "3  \" More I can't make any real suggestions on im...\n",
              "4  You, sir, are my hero. Any chance you remember..."
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train_df = pd.read_csv('/content/drive/MyDrive/NLP_Project_IITB/Project/jigsaw-toxic-comment-classification-challenge/train.csv')\n",
        "train_df.head(5)\n",
        "X = transform_x(train_df)\n",
        "X.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "YTphPVb-lyeE"
      },
      "outputs": [],
      "source": [
        "tf_idf = TfidfVectorizer()\n",
        "tf_idf.fit(X['comment_text'])\n",
        "max_idf = max(tf_idf.idf_)\n",
        "tf_idf_dict = defaultdict(\n",
        "            lambda: max_idf,\n",
        "            [(w, tf_idf.idf_[i]) for w, i in tf_idf.vocabulary_.items()])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U0qTtRUem5X5"
      },
      "outputs": [],
      "source": [
        "X_np_comment = X.apply(lambda row: get_sentence_embed_tf_idf(row['comment_text']),axis=1)\n",
        "X_train_np = np.stack(X_np_comment, axis=0)\n",
        "\n",
        "\n",
        "X_np_comment_features = X.apply(lambda row: get_sentence_embed_tf_idf_with_features(row['comment_text']),axis=1)\n",
        "X_train_np_features = np.stack(X_np_comment_features, axis=0)\n",
        "\n",
        "norm_list = feature_normalise_param(X)\n",
        "X_np_comment_features_norm = X.apply(lambda row: get_sentence_embed_tf_idf_with_features_norm(row['comment_text'],norm_list),axis=1)\n",
        "X_train_np_features_norm = np.stack(X_np_comment_features_norm, axis=0)\n",
        "\n",
        "print(X_train_np.shape, X_train_np_features.shape, X_train_np_features_norm.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ge1bz0ppYBxR",
        "outputId": "27634c1a-9ed7-48e3-d88c-524a1ceb7e27"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([0, 0, 0, ..., 0, 0, 0])"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "Y = combine_labels(train_df)\n",
        "Y.head()\n",
        "y_train_np = Y['Toxic'].to_numpy()\n",
        "y_train_np"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "olqdIjLj-ttA"
      },
      "source": [
        "# Model definition and training\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2WFEnvsNZCxR"
      },
      "outputs": [],
      "source": [
        "text_clf_tf_idf = Pipeline([\n",
        "                     ('clf', SVC(kernel='rbf', verbose=True) )\n",
        "                     ])\n",
        "text_clf_tf_idf_features = Pipeline([\n",
        "                     ('clf', SVC(kernel='rbf', verbose=True) )\n",
        "                     ])\n",
        "text_clf_tf_idf_features_norm = Pipeline([\n",
        "                     ('clf', SVC(kernel='rbf', verbose=True) )\n",
        "                     ])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rovD0BmyZFd0",
        "outputId": "770bd7eb-8e9c-4047-9359-8020a449d393"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[LibSVM]"
          ]
        },
        {
          "data": {
            "text/plain": [
              "Pipeline(steps=[('clf', SVC(verbose=True))])"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "text_clf_tf_idf.fit(X_train_np, y_train_np)\n",
        "text_clf_tf_idf_features.fit(X_train_np_features, y_train_np)\n",
        "text_clf_tf_idf_features_norm.fit(X_train_np_features_norm, y_train_np)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8B1hJ0MSkQBT"
      },
      "source": [
        "### Store as pickle objects to avoid retraining"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b0jHY86os4Al"
      },
      "outputs": [],
      "source": [
        "# pickle.dump(text_clf_tf_idf, open('/content/drive/MyDrive/NLP_Project_IITB/models/SVM_tf_idf.pkl','wb'))\n",
        "# pickle.dump(text_clf_tf_idf_features, open('/content/drive/MyDrive/NLP_Project_IITB/models/SVM_tf_idf_features.pkl','wb'))\n",
        "# pickle.dump(text_clf_tf_idf_features_norm, open('/content/drive/MyDrive/NLP_Project_IITB/models/SVM_tf_idf_features_norm.pkl','wb'))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q4-xe2md-meR"
      },
      "source": [
        "# Testing on Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "avuTk8GG-OsW",
        "outputId": "e3066601-5771-4cfc-f122-df6f34a4ee6f"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>comment_text</th>\n",
              "      <th>id</th>\n",
              "      <th>toxic</th>\n",
              "      <th>severe_toxic</th>\n",
              "      <th>obscene</th>\n",
              "      <th>threat</th>\n",
              "      <th>insult</th>\n",
              "      <th>identity_hate</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>Thank you for understanding. I think very high...</td>\n",
              "      <td>0001ea8717f6de06</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>:Dear god this site is horrible.</td>\n",
              "      <td>000247e83dcc1211</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>\"::: Somebody will invariably try to add Relig...</td>\n",
              "      <td>0002f87b16116a7f</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>\" It says it right there that it IS a type. Th...</td>\n",
              "      <td>0003e1cccfd5a40a</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>\" == Before adding a new product to the list, ...</td>\n",
              "      <td>00059ace3e3e9a53</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                         comment_text  ... identity_hate\n",
              "5   Thank you for understanding. I think very high...  ...             0\n",
              "7                    :Dear god this site is horrible.  ...             0\n",
              "11  \"::: Somebody will invariably try to add Relig...  ...             0\n",
              "13  \" It says it right there that it IS a type. Th...  ...             0\n",
              "14  \" == Before adding a new product to the list, ...  ...             0\n",
              "\n",
              "[5 rows x 8 columns]"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "test_df = pd.read_csv('/content/drive/MyDrive/NLP_Project_IITB/Project/jigsaw-toxic-comment-classification-challenge/test.csv')\n",
        "y_test = pd.read_csv('/content/drive/MyDrive/NLP_Project_IITB/Project/jigsaw-toxic-comment-classification-challenge/test_labels.csv')\n",
        "y_test.head(3)\n",
        "x_test = transform_x(test_df)\n",
        "df_col_merged = merge(x_test,y_test)\n",
        "df_col_merged.head()\n",
        "test_df = drop_faulty_rows(df_col_merged)\n",
        "test_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "18ykmFQFUe54"
      },
      "outputs": [],
      "source": [
        "#Training data for tf-idf\n",
        "X_test_np_comment = test_df.apply(lambda row: get_sentence_embed_tf_idf(row['comment_text']),axis=1)\n",
        "X_test_np = np.stack(X_test_np_comment, axis=0)\n",
        "\n",
        "#Training data for tf-idf with features\n",
        "X_test_np_comment_features = test_df.apply(lambda row: get_sentence_embed_tf_idf_with_features(row['comment_text']),axis=1)\n",
        "X_test_np_features = np.stack(X_test_np_comment_features, axis=0)\n",
        "\n",
        "#Training data for tf-idf with features normaliased\n",
        "X_test_np_comment_features_norm = test_df.apply(lambda row: get_sentence_embed_tf_idf_with_features_norm(row['comment_text'],norm_list),axis=1)\n",
        "X_test_np_features_norm = np.stack(X_test_np_comment_features_norm, axis=0)\n",
        "\n",
        "print(X_test_np.shape,X_test_np_features.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bkDcBnzmWjgO"
      },
      "outputs": [],
      "source": [
        "y_test_np = combine_labels(test_df)['Toxic'].to_numpy()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GaVbygIwhRgr"
      },
      "source": [
        "## Predictions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xRFDBMXFZhXl"
      },
      "outputs": [],
      "source": [
        "predicted_tf_idf = text_clf_tf_idf.predict(X_test_np)\n",
        "predicted_tf_idf_features = text_clf_tf_idf_features.predict(X_test_np_features)\n",
        "predicted_tf_idf_features_norm = text_clf_tf_idf_features_norm.predict(X_test_np_features_norm)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0rao1VY51X7o",
        "outputId": "573767a1-27c4-4caa-f7cf-8dc94f556778"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.95      0.97      0.96     57735\n",
            "           1       0.66      0.54      0.60      6243\n",
            "\n",
            "    accuracy                           0.93     63978\n",
            "   macro avg       0.81      0.76      0.78     63978\n",
            "weighted avg       0.92      0.93      0.93     63978\n",
            "\n"
          ]
        }
      ],
      "source": [
        "print(classification_report(y_test_np, predicted_tf_idf))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2hZLUZTk1XyR",
        "outputId": "cfb8e053-d80d-4213-caf2-4f1fe89a0714"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.93      0.99      0.96     57735\n",
            "           1       0.68      0.28      0.40      6243\n",
            "\n",
            "    accuracy                           0.92     63978\n",
            "   macro avg       0.80      0.63      0.68     63978\n",
            "weighted avg       0.90      0.92      0.90     63978\n",
            "\n"
          ]
        }
      ],
      "source": [
        "print(classification_report(y_test_np, predicted_tf_idf_features))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T_-Rv4Sk1bKq",
        "outputId": "d87da6fc-0dac-4cfb-a093-fe7bdabf85f3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.95      0.97      0.96     57735\n",
            "           1       0.66      0.49      0.57      6243\n",
            "\n",
            "    accuracy                           0.93     63978\n",
            "   macro avg       0.80      0.73      0.76     63978\n",
            "weighted avg       0.92      0.93      0.92     63978\n",
            "\n"
          ]
        }
      ],
      "source": [
        "print(classification_report(y_test_np, predicted_tf_idf_features_norm))\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "Toxic_features.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
