{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lLmApzq349rG"
      },
      "source": [
        "# Imports and installs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dlA1zvc34272",
        "outputId": "330dc7c5-f912-498e-e854-e6ac4b4bf373"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.7/dist-packages (2.1.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (4.62.3)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (3.2.5)\n",
            "Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (0.1.2)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.6.0 in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (4.12.5)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (1.19.5)\n",
            "Requirement already satisfied: tokenizers>=0.10.3 in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (0.10.3)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (0.1.96)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (1.0.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (1.4.1)\n",
            "Requirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (1.10.0+cu111)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (0.11.1+cu111)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.6.0->sentence-transformers) (3.10.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (21.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (2.23.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (4.8.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (2019.12.20)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (3.4.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (6.0)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (0.0.46)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers<5.0.0,>=4.6.0->sentence-transformers) (3.0.6)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers<5.0.0,>=4.6.0->sentence-transformers) (3.6.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from nltk->sentence-transformers) (1.15.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers<5.0.0,>=4.6.0->sentence-transformers) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers<5.0.0,>=4.6.0->sentence-transformers) (2021.10.8)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers<5.0.0,>=4.6.0->sentence-transformers) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers<5.0.0,>=4.6.0->sentence-transformers) (2.10)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers<5.0.0,>=4.6.0->sentence-transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers<5.0.0,>=4.6.0->sentence-transformers) (1.1.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->sentence-transformers) (3.0.0)\n",
            "Requirement already satisfied: pillow!=8.3.0,>=5.3.0 in /usr/local/lib/python3.7/dist-packages (from torchvision->sentence-transformers) (7.1.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install sentence-transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oQCU-59T4f1z",
        "outputId": "32572a12-9d22-4d19-eb8d-2e60fc3fa2d6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import nltk\n",
        "from nltk.corpus import wordnet\n",
        "import pandas as pd\n",
        "import random\n",
        "import numpy as np\n",
        "import sklearn.decomposition\n",
        "import matplotlib.pyplot as plt\n",
        "import pickle\n",
        "from collections import defaultdict\n",
        "import string\n",
        "import re\n",
        "from gensim.models import KeyedVectors\n",
        "from nltk import word_tokenize\n",
        "from tabulate import tabulate\n",
        "\n",
        "from sklearn.model_selection import train_test_split #split data into train and test sets\n",
        "from sklearn.feature_extraction.text import CountVectorizer #convert text comment into a numeric vector\n",
        "from sklearn.feature_extraction.text import TfidfTransformer, TfidfVectorizer #use TF IDF transformer to change text vector created by count vectorizer\n",
        "from sklearn.svm import SVC# Support Vector Machine\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.ensemble import BaggingClassifier, RandomForestClassifier, AdaBoostClassifier\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('punkt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "zYt87_kfW1gm"
      },
      "outputs": [],
      "source": [
        "from tensorflow import keras\n",
        "from keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation, GRU,Conv1D,MaxPooling1D, Flatten, GlobalMaxPooling1D\n",
        "from keras.models import Model,Sequential, load_model\n",
        "from keras.regularizers import l2\n",
        "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras import regularizers\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o8tXBc9O4vAK",
        "outputId": "b83eae60-6e00-4bf2-9bc6-2012f672a815"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9A4W43sI5MS4"
      },
      "source": [
        "#Feature vector generating functions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "taHjVCOu5t5V"
      },
      "source": [
        "###Extra Feature SVM models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 79,
      "metadata": {
        "id": "7JS4w38X47h-"
      },
      "outputs": [],
      "source": [
        "def preprocess_text(s):\n",
        "    s = s.replace('\\n',' ')\n",
        "    s = s.replace('\\t',' ')\n",
        "    s = s.replace(':',' ')\n",
        "    s = s.replace('#',' ')\n",
        "    s = s.replace('*','u')\n",
        "    s = s.replace('@','a')\n",
        "    s = s.replace('$','s')\n",
        "    s = s.replace('7','s')\n",
        "    s = s.replace('2','to')\n",
        "    s = s.replace('8','ight')\n",
        "    s = s.replace('&', 'and')\n",
        "    s = s.translate(str.maketrans('', '', string.punctuation) ) \n",
        "    s = s.split()\n",
        "    s = [i for i in s if i]\n",
        "    s = [re.sub(\"[^0-9a-zA-Z]+\", \"\", i) for i in s]\n",
        "    s = [i for i in s if len(i)>1]\n",
        "    s = \" \".join(s)\n",
        "    s = s.split()\n",
        "    \n",
        "    return \" \".join(s)\n",
        "\n",
        "def preprocess_text_for_features(s):\n",
        "    s = s.replace('\\n',' ')\n",
        "    s = s.replace('\\t',' ')\n",
        "    s = s.replace('7','s')\n",
        "    s = s.replace('2','to')\n",
        "    s = s.replace('8','ight')\n",
        "    s = s.split()\n",
        "    s = [i for i in s if i]\n",
        "    s = \" \".join(s)\n",
        "    s = s.split()\n",
        "    return \" \".join(s)\n",
        "\n",
        "def transform_x_for_features(df):\n",
        "    x = df.apply(lambda row : preprocess_text_for_features(row['comment_text']), axis=1)\n",
        "    return pd.DataFrame(x,columns=['comment_text'])\n",
        "\n",
        "def transform_x(df):\n",
        "    x = df.apply(lambda row : preprocess_text(row['comment_text']), axis=1)\n",
        "    return pd.DataFrame(x,columns=['comment_text'])\n",
        "\n",
        "def merge(df1,df2):\n",
        "    return pd.concat([df1, df2], axis=1)\n",
        "\n",
        "def drop_faulty_rows(df):\n",
        "    return df.drop(df[(df['toxic'] == -1.0) & (df['severe_toxic'] == -1.0) & \n",
        "                    (df['obscene'] == -1.0) & (df['threat'] == -1.0) & \n",
        "                    (df['insult'] == -1.0) & (df['identity_hate'] == -1.0) ].index)\n",
        "    \n",
        "def combine_labels(train_df):\n",
        "    x = np.where(train_df['toxic']+train_df['severe_toxic']+train_df['obscene']\n",
        "             +train_df['threat']+train_df['insult']+train_df['identity_hate'] > 0, 1, 0)\n",
        "    return pd.DataFrame(x,columns=['Toxic'])\n",
        "\n",
        "w2v_whole_data = KeyedVectors.load_word2vec_format('/content/drive/MyDrive/NLP_Project_IITB/Project/jigsaw-toxic-comment-classification-challenge/data/custom_glove_768d.txt')\n",
        "n_dim = 768\n",
        "\n",
        "def get_word_vec(word):\n",
        "    try:\n",
        "         return w2v_whole_data.word_vec(word)\n",
        "    except:\n",
        "        return np.zeros(n_dim) \n",
        "vect_get_word_vec = np.vectorize(get_word_vec)\n",
        "\n",
        "def get_sentence_embed(sent):\n",
        "    words = np.array(sent.split())\n",
        "    if len(words)==0:\n",
        "        return np.zeros(n_dim)\n",
        "    word_vecs = np.array([vect_get_word_vec(x) for x in words])\n",
        "    return np.average(word_vecs,axis=0)\n",
        "\n",
        "def get_sentence_embed_tf_idf(sent):\n",
        "    global tf_idf_dict\n",
        "    words = np.array(sent.split())\n",
        "    if len(words) == 0:\n",
        "        return np.zeros(n_dim)\n",
        "    word_vecs = np.array([vect_get_word_vec(x) for x in words])\n",
        "    for i in range(len(words)):\n",
        "        word_vecs[i] = tf_idf_dict[words[i]]*word_vecs[i]\n",
        "    return np.average(word_vecs,axis=0)\n",
        "\n",
        "stop_words = set(nltk.corpus.stopwords.words('english'))  \n",
        "\n",
        "def get_sentence_embed_tf_idf_with_features(sent):\n",
        "  global tf_idf_dict\n",
        "  words = np.array(sent.split())\n",
        "  if len(words) == 0:\n",
        "      return np.zeros(n_dim)\n",
        "  word_vecs = np.array([vect_get_word_vec(x) for x in words])\n",
        "  for i in range(len(words)):\n",
        "      word_vecs[i] = tf_idf_dict[words[i]]*word_vecs[i]\n",
        "  vec =  np.average(word_vecs,axis=0).tolist()\n",
        "\n",
        "  #Total Length of sentences\n",
        "  vec.append(len(sent))\n",
        "\n",
        "  #Total number of words\n",
        "  vec.append(len(words))\n",
        "\n",
        "  #Number of captial characters\n",
        "  caps = 0\n",
        "  for ch in sent:\n",
        "    if ch.isupper():\n",
        "      caps += 1\n",
        "  vec.append(caps)\n",
        "\n",
        "  #Number of all capital words\n",
        "  caps = 0\n",
        "  for ch in words:\n",
        "    if ch.isupper():\n",
        "      caps += 1\n",
        "  vec.append(caps)\n",
        "  \n",
        "  #Number of exclamation marks \n",
        "  vec.append(sent.count('!'))\n",
        "  \n",
        "\n",
        "  #Number of question marks \n",
        "  vec.append(sent.count('?'))\n",
        "\n",
        "\n",
        "  #Number of punctuations marks \n",
        "  vec.append(sent.count('.')+sent.count(',')+sent.count(';')+sent.count(':')) \n",
        "\n",
        "  #Number of symbols *&$% marks \n",
        "  vec.append(sent.count('*')+sent.count('&')+sent.count('$')+sent.count('%')) \n",
        "\n",
        "  #Number of words inside quotes single or double:\n",
        "  x = re.findall('\"([^\"]*)\"', sent)\n",
        "  y = re.findall(\"'([^']*)'\", sent)\n",
        "  vec.append(len(x)+len(y))\n",
        "\n",
        "  #Number of sentences:\n",
        "  vec.append(len(nltk.sent_tokenize(sent)))\n",
        "\n",
        "  #Count the number of unique words\n",
        "  vec.append(len(set(words)))\n",
        "\n",
        "  #Count of hashtags\n",
        "  vec.append(len(re.findall(r'(#[A-Za-z0-9]*)', sent)))\n",
        "\n",
        "\n",
        "  #Count of mentions\n",
        "  vec.append(len(re.findall(r'(@[A-Za-z0-9]*)', sent)))\n",
        "\n",
        "\n",
        "  #Count of stopwords\n",
        "  vec.append(len([w for w in words if w in stop_words]))\n",
        "  \n",
        "\n",
        "  #Calculating average word length\n",
        "  vec.append(len(sent)/len(words))\n",
        "\n",
        "  #Calculating average sentence length\n",
        "  vec.append(len(words)/len(nltk.sent_tokenize(sent)))\n",
        "  \n",
        "\n",
        "  #unique words vs word count feature\n",
        "  vec.append(len(set(words))/len(words))\n",
        "  \n",
        "\n",
        "  #Stopwords count vs words counts feature\n",
        "  vec.append(len([w for w in words if w in stop_words])/len(words))\n",
        "  \n",
        "  return np.array(vec)\n",
        "\n",
        "def feature_normalise_param(X):\n",
        "  res = []\n",
        "  Y = X\n",
        "  #Total Length of sentences\n",
        "  res.append(X['comment_text'].apply(len).max())\n",
        "\n",
        "  #Total number of words\n",
        "  res.append(X['comment_text'].apply(lambda x: len(x.split())).max())\n",
        "\n",
        "  #Number of captial characters\n",
        "  res.append(X['comment_text'].apply(lambda x: len(re.findall(\"([A-Z])\",x))).max())\n",
        "  \n",
        "  #Number of all capital words\n",
        "  res.append(X['comment_text'].apply(lambda x: len([1 for y in x.split() if y.isupper()])).max())\n",
        "  \n",
        "  #Number of exclamation marks \n",
        "  res.append(X['comment_text'].apply(lambda x: x.count('!')).max())\n",
        "\n",
        "  #Number of question marks \n",
        "  res.append(X['comment_text'].apply(lambda x: x.count('?')).max())\n",
        "\n",
        "  #Number of punctuations marks \n",
        "  res.append(X['comment_text'].apply(lambda x: x.count('.') + x.count(',') + x.count(';') + x.count(':')).max())\n",
        "\n",
        "  #Number of symbols *&$% marks \n",
        "  res.append(X['comment_text'].apply(lambda x: x.count('*') + x.count('&') + x.count('$') + x.count('%')).max())\n",
        "\n",
        "  #Number of words inside quotes single or double:\n",
        "  res.append(X['comment_text'].apply(lambda x: len(re.findall('\"([^\"]*)\"', x)) + len(re.findall(\"'([^']*)'\", x))).max())\n",
        "\n",
        "  #Number of sentences:\n",
        "  res.append(X['comment_text'].apply(lambda x: len(nltk.sent_tokenize(x))).max())\n",
        "\n",
        "  #Count the number of unique words\n",
        "  res.append(X['comment_text'].apply(lambda x: len(set(x.split()))).max())\n",
        "  \n",
        "  #Count of hashtags\n",
        "  res.append(X['comment_text'].apply(lambda x: len(re.findall(r'(#[A-Za-z0-9]*)', x))).max())\n",
        "\n",
        "  #Count of mentions\n",
        "  res.append(X['comment_text'].apply(lambda x: len(re.findall(r'(@[A-Za-z0-9]*)', x))).max())\n",
        "\n",
        "  #Count of stopwords\n",
        "  res.append(X['comment_text'].apply(lambda x: len([w for w in x.split() if w in stop_words])).max())\n",
        "  \n",
        "  return res\n",
        "\n",
        "def get_sentence_embed_tf_idf_with_features_norm(sent,maxs):\n",
        "  global tf_idf_dict\n",
        "  words = np.array(sent.split())\n",
        "  if len(words) == 0:\n",
        "      return np.zeros(n_dim)\n",
        "  word_vecs = np.array([vect_get_word_vec(x) for x in words])\n",
        "  for i in range(len(words)):\n",
        "      word_vecs[i] = tf_idf_dict[words[i]]*word_vecs[i]\n",
        "  vec =  np.average(word_vecs,axis=0).tolist()\n",
        "\n",
        "\n",
        "  #Total Length of sentences\n",
        "  vec.append(len(sent)/maxs[0])\n",
        "\n",
        "  #Total number of words\n",
        "  vec.append(len(words)/maxs[1])\n",
        "\n",
        "  #Number of captial characters\n",
        "  caps = 0\n",
        "  for ch in sent:\n",
        "    if ch.isupper():\n",
        "      caps += 1\n",
        "  vec.append(caps/maxs[2])\n",
        "\n",
        "  #Number of all capital words\n",
        "  caps = 0\n",
        "  for ch in words:\n",
        "    if ch.isupper():\n",
        "      caps += 1\n",
        "  vec.append(caps/maxs[3])\n",
        "  \n",
        "  #Number of exclamation marks \n",
        "  vec.append(sent.count('!')/maxs[4])\n",
        "  \n",
        "  #Number of question marks \n",
        "  vec.append(sent.count('?')/maxs[5])\n",
        "\n",
        "  #Number of punctuations marks \n",
        "  vec.append((sent.count('.')+sent.count(',')+sent.count(';')+sent.count(':'))/maxs[6]) \n",
        "\n",
        "  #Number of symbols *&$% marks \n",
        "  vec.append((sent.count('*')+sent.count('&')+sent.count('$')+sent.count('%'))/maxs[7]) \n",
        "\n",
        "  #Number of words inside quotes single or double:\n",
        "  x = re.findall('\"([^\"]*)\"', sent)\n",
        "  y = re.findall(\"'([^']*)'\", sent)\n",
        "  vec.append((len(x)+len(y))/maxs[8])\n",
        "\n",
        "  #Number of sentences:\n",
        "  vec.append(len(nltk.sent_tokenize(sent))/maxs[9])\n",
        "\n",
        "  #Count the number of unique words\n",
        "  vec.append(len(set(words))/maxs[10])\n",
        "\n",
        "  #Count of hashtags\n",
        "  vec.append(len(re.findall(r'(#[A-Za-z0-9]*)', sent))/maxs[11])\n",
        "\n",
        "  #Count of mentions\n",
        "  vec.append(len(re.findall(r'(@[A-Za-z0-9]*)', sent))/maxs[12])\n",
        "\n",
        "  #Count of stopwords\n",
        "  vec.append(len([w for w in words if w in stop_words])/maxs[13])\n",
        "  \n",
        "  #Calculating average word length\n",
        "  vec.append(len(sent)/len(words))\n",
        "\n",
        "  #Calculating average sentence length\n",
        "  vec.append(len(words)/len(nltk.sent_tokenize(sent)))\n",
        "  \n",
        "\n",
        "  #unique words vs word count feature\n",
        "  vec.append(len(set(words))/len(words))\n",
        "  \n",
        "\n",
        "  #Stopwords count vs words counts feature\n",
        "  vec.append(len([w for w in words if w in stop_words])/len(words))\n",
        "  \n",
        "  return np.array(vec)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 80,
      "metadata": {
        "id": "sovMFTPM8Bh6"
      },
      "outputs": [],
      "source": [
        "def vectorise_tf_idf_1(sent):\n",
        "  sent = preprocess_text_for_features(sent)\n",
        "  return np.reshape(np.stack(get_sentence_embed_tf_idf(sent), axis=0), (1, -1))\n",
        "\n",
        "def vectorise_tf_idf_with_features_1(sent):\n",
        "  sent = preprocess_text_for_features(sent)\n",
        "  return np.reshape(np.stack(get_sentence_embed_tf_idf_with_features(sent), axis=0), (1, -1))\n",
        "\n",
        "norm_list=[5233, 1411, 4960, 1352, 4942, 209, 682, 148, 235, 683, 816, 72, 128, 887]\n",
        "def vectorise_tf_idf_with_features_norm_1(sent):\n",
        "  sent = preprocess_text_for_features(sent)\n",
        "  return np.reshape(np.stack(get_sentence_embed_tf_idf_with_features_norm(sent,norm_list), axis=0), (1, -1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "5kSyxzOl9Pyb"
      },
      "outputs": [],
      "source": [
        "train_df = pd.read_csv('/content/drive/MyDrive/NLP_Project_IITB/Project/jigsaw-toxic-comment-classification-challenge/train.csv')\n",
        "train_df.head(5)\n",
        "X = transform_x(train_df)\n",
        "X.head()\n",
        "tf_idf = TfidfVectorizer()\n",
        "tf_idf.fit(X['comment_text'])\n",
        "max_idf = max(tf_idf.idf_)\n",
        "tf_idf_dict = defaultdict(\n",
        "            lambda: max_idf,\n",
        "            [(w, tf_idf.idf_[i]) for w, i in tf_idf.vocabulary_.items()])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KX9DcYea50tR"
      },
      "source": [
        "### SVM with Custom Embedding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 81,
      "metadata": {
        "id": "zaVhKDTw5o7_"
      },
      "outputs": [],
      "source": [
        "w2v = KeyedVectors.load_word2vec_format('/content/drive/MyDrive/NLP_Project_IITB/Project/jigsaw-toxic-comment-classification-challenge/data/custom_glove_768d.txt')\n",
        "n_dim = 768\n",
        "\n",
        "def get_word_vec_2(word):\n",
        "    try:\n",
        "         return w2v.word_vec(word)\n",
        "    except:\n",
        "        return np.zeros(n_dim) \n",
        "vect_get_word_vec_2 = np.vectorize(get_word_vec_2)\n",
        "\n",
        "\n",
        "def get_sentence_embed_2(sent):\n",
        "    words = np.array(sent.split())\n",
        "    if len(words)==0:\n",
        "        return np.zeros(n_dim)\n",
        "    word_vecs = np.array([vect_get_word_vec_2(x) for x in words])\n",
        "    return np.average(word_vecs,axis=0)\n",
        "\n",
        "def get_sentence_embed_tf_idf_2(sent):\n",
        "    global tf_idf_dict\n",
        "    words = np.array(sent.split())\n",
        "    if len(words) == 0:\n",
        "        return np.zeros(n_dim)\n",
        "    word_vecs = np.array([vect_get_word_vec_2(x) for x in words])\n",
        "    for i in range(len(words)):\n",
        "        word_vecs[i] = tf_idf_dict[words[i]]*word_vecs[i]\n",
        "    return np.average(word_vecs,axis=0)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "4U3JRO1v9EiD"
      },
      "outputs": [],
      "source": [
        "def vectorise_2(sent):\n",
        "  sent = preprocess_text(sent)\n",
        "  return np.reshape(np.stack(get_sentence_embed_2(sent), axis=0), (1, -1))\n",
        "\n",
        "def vectorise_tf_idf_2(sent):\n",
        "  sent = preprocess_text(sent)\n",
        "  return np.reshape(np.stack(get_sentence_embed_tf_idf_2(sent), axis=0), (1, -1))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "42IGJponCxyX"
      },
      "source": [
        "### SVM using Doc_2_vec"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "5qrD5yjhCelu"
      },
      "outputs": [],
      "source": [
        "from gensim.models.doc2vec import Doc2Vec\n",
        "doc2vec = Doc2Vec.load(\"/content/drive/MyDrive/NLP_Project_IITB/Project/jigsaw-toxic-comment-classification-challenge/data/doc2vec_model\")\n",
        "\n",
        "def vectorise_doc2vec_3(sent):\n",
        "  sent = preprocess_text(sent)\n",
        "  return np.reshape(np.stack(doc2vec.infer_vector(word_tokenize(sent)), axis=0), (1, -1))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BnmIduXzFXom"
      },
      "source": [
        "### SVM with Count Vectoriser"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "Y9hlrKteFFDi"
      },
      "outputs": [],
      "source": [
        "def vectorise_doc2vec_4(sent):\n",
        "  sent = preprocess_text(sent)\n",
        "  return np.array([sent])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CScJO2R8Wghd"
      },
      "source": [
        "### SVM using NN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "uawXBjl4WniJ"
      },
      "outputs": [],
      "source": [
        "def custom_predict_5(classifier, x_t, batch_size=32, extractor=None, kernel_approximation=None, reshapeLayer=None):\n",
        "    y_pred = np.array([])\n",
        "    def pred_batch(iterable, n=1):\n",
        "        l = len(iterable)\n",
        "        for ndx in range(0, l, n):\n",
        "            yield iterable[ndx:min(ndx + n, l)]\n",
        "    i = 0\n",
        "    for batch_range in pred_batch(range(0, len(x_t)), batch_size):\n",
        "        if extractor is not None:\n",
        "            x_t_b = extractor(x_t[batch_range.start: batch_range.stop]).numpy()\n",
        "            if kernel_approximation is not None:\n",
        "                x_t_b = feature_map_nystroem.fit_transform(x_t_b)\n",
        "            '''\n",
        "            reshape if extractor is gives more than 2 dims (like in conv1d)\n",
        "            '''\n",
        "            if reshapeLayer is not None:\n",
        "                x_t_b = x_t_b.reshape(x_t_b.shape[0],-1)\n",
        "        else:\n",
        "            x_t_b = x_t[batch_range.start: batch_range.stop]\n",
        "        y_t = classifier.predict(x_t_b)\n",
        "        y_pred = np.hstack((y_pred,y_t)).ravel()\n",
        "    return y_pred"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "XgDEPPF3Wesp"
      },
      "outputs": [],
      "source": [
        "tokenizer = pickle.load(open('/content/drive/MyDrive/NLP_Project_IITB/Project/jigsaw-toxic-comment-classification-challenge/data/tokenizer.pkl','rb'))\n",
        "maxlen = 500\n",
        "# for samples < n_components of kernel approximation Nystroem cannot be done\n",
        "def prediction_5(model, sent, extractor, reshapeLayer=None):\n",
        "    sents = pad_sequences(tokenizer.texts_to_sequences([sent]), maxlen=\n",
        "                          maxlen,padding='post')\n",
        "    sent_pred = custom_predict_5(model, sents, extractor=extractor, reshapeLayer = reshapeLayer)\n",
        "    if sent_pred[0] == 1: return 'Toxic'\n",
        "    return 'Not Toxic'\n",
        "\n",
        "\n",
        "def sgd_prediction_5(model, sent):\n",
        "    sents = pad_sequences(tokenizer.texts_to_sequences([sent]), maxlen=\n",
        "                          maxlen,padding='post')\n",
        "    res = model.predict(sents)\n",
        "    if res[0] == 1: return 'Toxic'\n",
        "    return 'Not Toxic'\n",
        "\n",
        "\n",
        "# Model 1-Flatten\n",
        "model1_flatten = load_model('/content/drive/MyDrive/NLP_Project_IITB/Project/training_models/text_feature_extractor-ep003-loss0.202-acc0.899-val_loss0.202-val_acc0.899.hdf5')\n",
        "layer_name = \"flatten\"\n",
        "extractor_model1_flatten = Model(inputs=model1_flatten.inputs,\n",
        "                        outputs=model1_flatten.get_layer(layer_name).output)\n",
        "model1_flatten_svm = pickle.load(open('/content/drive/MyDrive/NLP_Project_IITB/Project/training_models/svm-sgd-model1-no-kernel-extractor-flatten.pkl','rb'))\n",
        "\n",
        "\n",
        "# Model 1-Oversampled\n",
        "model1_oversampled_flatten = load_model('/content/drive/MyDrive/NLP_Project_IITB/Project/training_models/over-sampled-text_feature_extractor-ep001-loss0.709-acc0.645-val_loss0.203-val_acc0.898.hdf5')\n",
        "layer_name = 'flatten'\n",
        "extractor_model1_oversampled_flatten = Model(inputs=model1_oversampled_flatten.inputs,\n",
        "                        outputs=model1_oversampled_flatten.get_layer(layer_name).output)\n",
        "model1_oversampled_flatten_svm = pickle.load(open('/content/drive/MyDrive/NLP_Project_IITB/Project/training_models/svm-sgd-model1-oversampled-no-kernel-extractor-flatten.pkl','rb'))\n",
        "\n",
        " \n",
        "# Model 3-Flatten\n",
        "model3_flatten = load_model('/content/drive/MyDrive/NLP_Project_IITB/Project/training_models/Model-Binary-loss-gpu-ep002-loss0.013-acc0.985-val_loss0.277-val_acc0.934.hdf5')\n",
        "layer_name = \"flatten\"\n",
        "extractor_model3_flatten = Model(inputs=model3_flatten.inputs,\n",
        "                        outputs=model3_flatten.get_layer(layer_name).output)\n",
        "model3_flatten_svm = pickle.load(open('/content/drive/MyDrive/NLP_Project_IITB/Project/training_models/svm-sgd-model3-extractor-flatten.pkl','rb'))\n",
        "\n",
        "\n",
        "#Model 3-Conv1d\n",
        "model3_conv1d = load_model('/content/drive/MyDrive/NLP_Project_IITB/Project/training_models/Model-Binary-loss-gpu-ep002-loss0.013-acc0.985-val_loss0.277-val_acc0.934.hdf5')\n",
        "layer_name = \"conv1D\"\n",
        "extractor_model3_conv1d = Model(inputs=model3_conv1d.inputs,\n",
        "                        outputs=model3_conv1d.get_layer(layer_name).output)\n",
        "model3_conv1d_svm =  pickle.load(open('/content/drive/MyDrive/NLP_Project_IITB/Project/training_models/svm-sgd-model3-extractor-conv1D.pkl','rb'))\n",
        "\n",
        "# SGD Classifier\n",
        "sgd_classifier = pickle.load(open('/content/drive/MyDrive/NLP_Project_IITB/Project/models/sgd_no-oversampling.pkl','rb'))\n",
        "\n",
        "# SGD Oversampled Classifier\n",
        "sgd_classifier_oversample = pickle.load(open('/content/drive/MyDrive/NLP_Project_IITB/Project/models/sgd_oversampled.pkl','rb'))\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mGOoYR_P36eC"
      },
      "source": [
        "### SVM Models using Sentence Embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "dTf5w5WW4HHa"
      },
      "outputs": [],
      "source": [
        "with open('/content/drive/MyDrive/NLP_Project_IITB/Project/models/sbert_model.pkl','rb') as handle: \n",
        "    sbert_model = pickle.load(handle) \n",
        "def vectoriser_6(sent): \n",
        "    return sbert_model.encode([preprocess_text(sent)]) "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rPfNacJ4G4Nq"
      },
      "source": [
        "# Loading all the SVM pre-trained Models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "NcnaRwfJG8Bq"
      },
      "outputs": [],
      "source": [
        "#---Extra Features SVM Models---\n",
        "SVM_rbf_custom_embedding_tf_idf_without_features = pickle.load(open('/content/drive/MyDrive/NLP_Project_IITB/models/SVM_tf_idf.pkl','rb'))\n",
        "SVM_rbf_custom_embedding_tf_idf_with_features = pickle.load(open('/content/drive/MyDrive/NLP_Project_IITB/models/SVM_tf_idf_features.pkl','rb'))\n",
        "SVM_rbf_custom_embedding_tf_idf_with_features_normalised = pickle.load(open('/content/drive/MyDrive/NLP_Project_IITB/models/SVM_tf_idf_features_norm.pkl','rb'))\n",
        "\n",
        "#---Custom Embeddings SVM Models---\n",
        "linear_svm_avg_custom_word2vec_embedding = pickle.load(open('/content/drive/MyDrive/NLP_Project_IITB/Project/models/lsvm_emb_on_Train_avged','rb'))\n",
        "linear_svm_avg_tfidf_custom_word2vec_embedding = pickle.load(open('/content/drive/MyDrive/NLP_Project_IITB/Project/models/lsvm_emb_on_Train_tfidf_avged','rb'))\n",
        "svm_rbf_avg_custom_word2vec_embedding = pickle.load(open('/content/drive/MyDrive/NLP_Project_IITB/Project/models/svm_rbf_emb_on_Train_avged','rb'))\n",
        "sgd_classifier_avg_custom_word2vec_embedding = pickle.load(open('/content/drive/MyDrive/NLP_Project_IITB/Project/models/sgd_emb_on_Train_avged','rb'))\n",
        "sgd_classifier_avg_tfidf_custom_word2vec_embedding = pickle.load(open('/content/drive/MyDrive/NLP_Project_IITB/Project/models/sgd_emb_on_Train_tfidf_avged','rb'))\n",
        "\n",
        "#---Doc2Vec Embeddings SVM Models---\n",
        "svm_rbf_doc2vec_custom_embedding = pickle.load(open('/content/drive/MyDrive/NLP_Project_IITB/Project/models/svm_rbf_d2v_emb.pkl','rb'))\n",
        "sgd_doc2vec_custom_embedding = pickle.load(open('/content/drive/MyDrive/NLP_Project_IITB/Project/models/sgd_d2v_emb.pkl','rb'))\n",
        "\n",
        "#---Count Vectoriser SVM Models---\n",
        "linear_svm_countVectorizer_tfdif = pickle.load(open('/content/drive/MyDrive/NLP_Project_IITB/Project/models/linearSVM_countvec_tfidf.pkl','rb'))\n",
        "linear_svm_countVectorizer_tfdif_nystroem_approximation = pickle.load(open('/content/drive/MyDrive/NLP_Project_IITB/Project/models/'+'nystroem_n_comp{}'.format(1000),'rb'))\n",
        "linear_svm_countVectorizer_tfdif_fourier_approximation = pickle.load(open('/content/drive/MyDrive/NLP_Project_IITB/Project/models/'+'fourier_n_comp{}'.format(1000),'rb'))\n",
        "svm_rbf_countVectorizer_tfdif = pickle.load(open('/content/drive/MyDrive/NLP_Project_IITB/Project/models/svm-test-train-count_vec_pipeline.pkl','rb'))\n",
        "sgd_svm_countVectorizer_tfdif = pickle.load(open('/content/drive/MyDrive/NLP_Project_IITB/Project/models/sgdSVM-test-train-count_vec_pipeline.pkl','rb'))\n",
        "sgd_svm_countVectorizer_tfdif_nystroem_approximation = pickle.load(open('/content/drive/MyDrive/NLP_Project_IITB/Project/models/sgd_nystroem_approx_svm-test-train-count_vec_pipeline.pkl','rb'))\n",
        "sgd_svm_countVectorizer_tfdif_fourier_approximation = pickle.load(open('/content/drive/MyDrive/NLP_Project_IITB/Project/models/sgd_fourier_approx_svm-test-train-count_vec_pipeline.pkl','rb'))\n",
        "\n",
        "#---SVM Models using bert-base-nli-mean-tokens sentence Embeddings---\n",
        "SVM_rbf_sentence_embedding = pickle.load(open('/content/drive/MyDrive/NLP_Project_IITB/Project/models/svm-embedding-rbf.pkl','rb'))\n",
        "SVM_sigmoid_sentence_embedding = pickle.load(open('/content/drive/MyDrive/NLP_Project_IITB/Project/models/svm-embedding-sigmoid.pkl','rb'))\n",
        "SVM_polynomial_sentence_embedding = pickle.load(open('/content/drive/MyDrive/NLP_Project_IITB/Project/models/svm-embedding-poly.pkl','rb'))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P5l_6x_OKHft"
      },
      "source": [
        "#Prediction function to get output from all the SVM models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 90,
      "metadata": {
        "id": "nMydd_zbKLsT"
      },
      "outputs": [],
      "source": [
        "def Predict(sent):\n",
        "  res = []\n",
        "  toxic_count = 0\n",
        "  if (SVM_rbf_custom_embedding_tf_idf_without_features.predict(vectorise_tf_idf_1(sent))):\n",
        "    res.append([\"SVM_rbf_custom_embedding_tf_idf_without_features\",\"Toxic\"])\n",
        "    toxic_count += 1\n",
        "  else:\n",
        "    res.append([\"SVM_rbf_custom_embedding_tf_idf_without_features\",\"Not Toxic\"])\n",
        "\n",
        "  if (SVM_rbf_custom_embedding_tf_idf_with_features.predict(vectorise_tf_idf_with_features_1(sent))):\n",
        "    res.append([\"SVM_rbf_custom_embedding_tf_idf_with_features\",\"Toxic\"])\n",
        "    toxic_count += 1\n",
        "  else:\n",
        "    res.append([\"SVM_rbf_custom_embedding_tf_idf_with_features\",\"Not Toxic\"])\n",
        "\n",
        "  if (SVM_rbf_custom_embedding_tf_idf_with_features_normalised.predict(vectorise_tf_idf_with_features_norm_1(sent))):\n",
        "    res.append([\"SVM_rbf_custom_embedding_tf_idf_with_features_normalised\",\"Toxic\"])\n",
        "    toxic_count += 1\n",
        "  else:\n",
        "    res.append([\"SVM_rbf_custom_embedding_tf_idf_with_features_normalised\",\"Not Toxic\"])\n",
        "  \n",
        "  #--------------------SVM Custom Embedding Models-----------------------#\n",
        "  if (linear_svm_avg_custom_word2vec_embedding.predict(vectorise_2(sent))):\n",
        "    res.append([\"linear_svm_avg_custom_word2vec_embedding\",\"Toxic\"])\n",
        "    toxic_count += 1\n",
        "  else:\n",
        "    res.append([\"linear_svm_avg_custom_word2vec_embedding\",\"Not Toxic\"])\n",
        "\n",
        "  if (linear_svm_avg_tfidf_custom_word2vec_embedding.predict(vectorise_tf_idf_2(sent))):\n",
        "    res.append([\"linear_svm_avg_tfidf_custom_word2vec_embedding\",\"Toxic\"])\n",
        "    toxic_count += 1\n",
        "  else:\n",
        "    res.append([\"linear_svm_avg_tfidf_custom_word2vec_embedding\",\"Not Toxic\"])\n",
        "\n",
        "  if (svm_rbf_avg_custom_word2vec_embedding.predict(vectorise_2(sent))):\n",
        "    res.append([\"svm_rbf_avg_custom_word2vec_embedding\",\"Toxic\"])\n",
        "    toxic_count += 1\n",
        "  else:\n",
        "    res.append([\"svm_rbf_avg_custom_word2vec_embedding\",\"Not Toxic\"])\n",
        "\n",
        "\n",
        "  if (sgd_classifier_avg_custom_word2vec_embedding.predict(vectorise_2(sent))):\n",
        "    res.append([\"sgd_classifier_avg_custom_word2vec_embedding\",\"Toxic\"])\n",
        "    toxic_count += 1\n",
        "  else:\n",
        "    res.append([\"sgd_classifier_avg_custom_word2vec_embedding\",\"Not Toxic\"])\n",
        "\n",
        "  if (sgd_classifier_avg_tfidf_custom_word2vec_embedding.predict(vectorise_tf_idf_2(sent))):\n",
        "    res.append([\"sgd_classifier_avg_tfidf_custom_word2vec_embedding\",\"Toxic\"])\n",
        "    toxic_count += 1\n",
        "  else:\n",
        "    res.append([\"sgd_classifier_avg_tfidf_custom_word2vec_embedding\",\"Not Toxic\"])\n",
        "\n",
        "\n",
        " #--------------------SVM Doc2Vec Models-----------------------#\n",
        "\n",
        "\n",
        "  if (svm_rbf_doc2vec_custom_embedding.predict(vectorise_doc2vec_3(sent))):\n",
        "    res.append([\"svm_rbf_doc2vec_custom_embedding\",\"Toxic\"])\n",
        "    toxic_count += 1\n",
        "  else:\n",
        "    res.append([\"svm_rbf_doc2vec_custom_embedding\",\"Not Toxic\"])\n",
        "\n",
        "  if (sgd_doc2vec_custom_embedding.predict(vectorise_doc2vec_3(sent))):\n",
        "    res.append([\"sgd_doc2vec_custom_embedding\",\"Toxic\"])\n",
        "    toxic_count += 1\n",
        "  else:\n",
        "    res.append([\"sgd_doc2vec_custom_embedding\",\"Not Toxic\"])\n",
        "\n",
        "\n",
        "  #--------------------SVM Count Vectoriser Models-----------------------#\n",
        "\n",
        "  if (linear_svm_countVectorizer_tfdif.predict(vectorise_doc2vec_4(sent))):\n",
        "    res.append([\"linear_svm_countVectorizer_tfdif\",\"Toxic\"])\n",
        "    toxic_count += 1\n",
        "  else:\n",
        "    res.append([\"linear_svm_countVectorizer_tfdif\",\"Not Toxic\"])\n",
        "\n",
        "  if (linear_svm_countVectorizer_tfdif_nystroem_approximation.predict(vectorise_doc2vec_4(sent))):\n",
        "    res.append([\"linear_svm_countVectorizer_tfdif_nystroem_approximation\",\"Toxic\"])\n",
        "    toxic_count += 1\n",
        "  else:\n",
        "    res.append([\"linear_svm_countVectorizer_tfdif_nystroem_approximation\",\"Not Toxic\"])\n",
        "\n",
        "  if (linear_svm_countVectorizer_tfdif_fourier_approximation.predict(vectorise_doc2vec_4(sent))):\n",
        "    res.append([\"linear_svm_countVectorizer_tfdif_fourier_approximation\",\"Toxic\"])\n",
        "    toxic_count += 1\n",
        "  else:\n",
        "    res.append([\"linear_svm_countVectorizer_tfdif_fourier_approximation\",\"Not Toxic\"])\n",
        "\n",
        "  if (svm_rbf_countVectorizer_tfdif.predict(vectorise_doc2vec_4(sent))):\n",
        "    res.append([\"svm_rbf_countVectorizer_tfdif\",\"Toxic\"])\n",
        "    toxic_count += 1\n",
        "  else:\n",
        "    res.append([\"svm_rbf_countVectorizer_tfdif\",\"Not Toxic\"])\n",
        "\n",
        "  if (sgd_svm_countVectorizer_tfdif.predict(vectorise_doc2vec_4(sent))):\n",
        "    res.append([\"sgd_svm_countVectorizer_tfdif\",\"Toxic\"])\n",
        "    toxic_count += 1\n",
        "  else:\n",
        "    res.append([\"sgd_svm_countVectorizer_tfdif\",\"Not Toxic\"])\n",
        "\n",
        "  if (sgd_svm_countVectorizer_tfdif_nystroem_approximation.predict(vectorise_doc2vec_4(sent))):\n",
        "    res.append([\"sgd_svm_countVectorizer_tfdif_nystroem_approximation\",\"Toxic\"])\n",
        "    toxic_count += 1\n",
        "  else:\n",
        "    res.append([\"sgd_svm_countVectorizer_tfdif_nystroem_approximation\",\"Not Toxic\"])\n",
        "\n",
        "  if (sgd_svm_countVectorizer_tfdif_fourier_approximation.predict(vectorise_doc2vec_4(sent))):\n",
        "    res.append([\"sgd_svm_countVectorizer_tfdif_fourier_approximation\",\"Toxic\"])\n",
        "    toxic_count += 1\n",
        "  else:\n",
        "    res.append([\"sgd_svm_countVectorizer_tfdif_fourier_approximation\",\"Not Toxic\"])\n",
        "  \n",
        "  #--------------------SVM NN Models-----------------------#\n",
        "\n",
        "  pred = prediction_5(model1_flatten_svm,sent,extractor_model1_flatten)\n",
        "  if(pred=='Toxic'):\n",
        "    toxic_count += 1\n",
        "  res.append([\"model1_flatten_svm\",pred])\n",
        "  \n",
        "  pred = prediction_5(model1_oversampled_flatten_svm,sent,extractor_model1_oversampled_flatten)\n",
        "  if(pred=='Toxic'):\n",
        "    toxic_count += 1\n",
        "  res.append([\"model1_oversampled_flatten_svm\",pred])\n",
        "  \n",
        "  pred = prediction_5(model3_flatten_svm,sent,extractor_model3_flatten)\n",
        "  if(pred=='Toxic'):\n",
        "    toxic_count += 1\n",
        "  res.append([\"model3_flatten_svm\",pred])\n",
        "  \n",
        "  pred = prediction_5(model3_conv1d_svm,sent,extractor_model3_conv1d, reshapeLayer=True)\n",
        "  if(pred=='Toxic'):\n",
        "    toxic_count += 1\n",
        "  res.append([\"model3_conv1d_svm\",pred])\n",
        "\n",
        "  #--------------------SGD Model on NN preprocessing setting-----------------------#\n",
        "\n",
        "  pred = sgd_prediction_5(sgd_classifier,sent)\n",
        "  if(pred=='Toxic'):\n",
        "    toxic_count += 1\n",
        "  res.append([\"sgd_classifier_with_NN_preprocessing\",pred])\n",
        "  \n",
        "  pred = sgd_prediction_5(sgd_classifier_oversample,sent)\n",
        "  if(pred=='Toxic'):\n",
        "    toxic_count += 1\n",
        "    res.append([\"sgd_classifier_oversample_with_NN_preprocessing\",pred])\n",
        "\n",
        "  #--------------------SVM Models using Sentence Embeddings-----------------------#\n",
        "\n",
        "  if (SVM_rbf_sentence_embedding.predict(vectoriser_6(sent))):\n",
        "    res.append([\"SVM_rbf_sentence_embedding\",\"Toxic\"])\n",
        "    toxic_count += 1\n",
        "  else:\n",
        "    res.append([\"SVM_rbf_sentence_embedding\",\"Not Toxic\"])\n",
        "\n",
        "  if (SVM_sigmoid_sentence_embedding.predict(vectoriser_6(sent))):\n",
        "    res.append([\"SVM_sigmoid_sentence_embedding\",\"Toxic\"])\n",
        "    toxic_count += 1\n",
        "  else:\n",
        "    res.append([\"SVM_sigmoid_sentence_embedding\",\"Not Toxic\"])\n",
        "\n",
        "  if (SVM_polynomial_sentence_embedding.predict(vectoriser_6(sent))):\n",
        "    res.append([\"SVM_polynomial_sentence_embedding\",\"Toxic\"])\n",
        "    toxic_count += 1\n",
        "  else:\n",
        "    res.append([\"SVM_polynomial_sentence_embedding\",\"Not Toxic\"])\n",
        "\n",
        "  print(tabulate(res, headers=['Model', 'Prediction']))\n",
        "  \n",
        "  \n",
        "  print(\"\\nTotal Toxic Predictions = \",toxic_count)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "apis__aHSQx7"
      },
      "source": [
        "#Analysis for ad-hoc sentences"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 91,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PHTSQqb7KoUF",
        "outputId": "e2d9d174-353c-4b60-bb58-2aca59726a1f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model                                                     Prediction\n",
            "--------------------------------------------------------  ------------\n",
            "SVM_rbf_custom_embedding_tf_idf_without_features          Not Toxic\n",
            "SVM_rbf_custom_embedding_tf_idf_with_features             Not Toxic\n",
            "SVM_rbf_custom_embedding_tf_idf_with_features_normalised  Not Toxic\n",
            "linear_svm_avg_custom_word2vec_embedding                  Toxic\n",
            "linear_svm_avg_tfidf_custom_word2vec_embedding            Toxic\n",
            "svm_rbf_avg_custom_word2vec_embedding                     Not Toxic\n",
            "sgd_classifier_avg_custom_word2vec_embedding              Toxic\n",
            "sgd_classifier_avg_tfidf_custom_word2vec_embedding        Toxic\n",
            "svm_rbf_doc2vec_custom_embedding                          Not Toxic\n",
            "sgd_doc2vec_custom_embedding                              Not Toxic\n",
            "linear_svm_countVectorizer_tfdif                          Not Toxic\n",
            "linear_svm_countVectorizer_tfdif_nystroem_approximation   Toxic\n",
            "linear_svm_countVectorizer_tfdif_fourier_approximation    Not Toxic\n",
            "svm_rbf_countVectorizer_tfdif                             Not Toxic\n",
            "sgd_svm_countVectorizer_tfdif                             Not Toxic\n",
            "sgd_svm_countVectorizer_tfdif_nystroem_approximation      Toxic\n",
            "sgd_svm_countVectorizer_tfdif_fourier_approximation       Not Toxic\n",
            "model1_flatten_svm                                        Not Toxic\n",
            "model1_oversampled_flatten_svm                            Toxic\n",
            "model3_flatten_svm                                        Toxic\n",
            "model3_conv1d_svm                                         Toxic\n",
            "sgd_classifier_with_NN_preprocessing                      Not Toxic\n",
            "sgd_classifier_oversample_with_NN_preprocessing           Toxic\n",
            "SVM_rbf_sentence_embedding                                Toxic\n",
            "SVM_sigmoid_sentence_embedding                            Toxic\n",
            "SVM_polynomial_sentence_embedding                         Toxic\n",
            "\n",
            "Total Toxic Predictions =  13\n"
          ]
        }
      ],
      "source": [
        "# NN capture even typos of abusive words better\n",
        "Predict(\"ffffffuuuuucccckkkkk off\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 92,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cK96qzzKoWZM",
        "outputId": "65683087-0f49-43a6-d85b-d37ae184a8b2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model                                                     Prediction\n",
            "--------------------------------------------------------  ------------\n",
            "SVM_rbf_custom_embedding_tf_idf_without_features          Toxic\n",
            "SVM_rbf_custom_embedding_tf_idf_with_features             Toxic\n",
            "SVM_rbf_custom_embedding_tf_idf_with_features_normalised  Toxic\n",
            "linear_svm_avg_custom_word2vec_embedding                  Toxic\n",
            "linear_svm_avg_tfidf_custom_word2vec_embedding            Toxic\n",
            "svm_rbf_avg_custom_word2vec_embedding                     Toxic\n",
            "sgd_classifier_avg_custom_word2vec_embedding              Toxic\n",
            "sgd_classifier_avg_tfidf_custom_word2vec_embedding        Toxic\n",
            "svm_rbf_doc2vec_custom_embedding                          Not Toxic\n",
            "sgd_doc2vec_custom_embedding                              Not Toxic\n",
            "linear_svm_countVectorizer_tfdif                          Toxic\n",
            "linear_svm_countVectorizer_tfdif_nystroem_approximation   Toxic\n",
            "linear_svm_countVectorizer_tfdif_fourier_approximation    Toxic\n",
            "svm_rbf_countVectorizer_tfdif                             Toxic\n",
            "sgd_svm_countVectorizer_tfdif                             Toxic\n",
            "sgd_svm_countVectorizer_tfdif_nystroem_approximation      Toxic\n",
            "sgd_svm_countVectorizer_tfdif_fourier_approximation       Toxic\n",
            "model1_flatten_svm                                        Not Toxic\n",
            "model1_oversampled_flatten_svm                            Not Toxic\n",
            "model3_flatten_svm                                        Toxic\n",
            "model3_conv1d_svm                                         Toxic\n",
            "sgd_classifier_with_NN_preprocessing                      Not Toxic\n",
            "SVM_rbf_sentence_embedding                                Toxic\n",
            "SVM_sigmoid_sentence_embedding                            Not Toxic\n",
            "SVM_polynomial_sentence_embedding                         Toxic\n",
            "\n",
            "Total Toxic Predictions =  19\n"
          ]
        }
      ],
      "source": [
        "#Conextual information is not well captured and words by themselves dominate the toxic labels.\n",
        "Predict(\"Fuck off is such a derogatory term and should not be used.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 84,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mH4uJj2vD7Qj",
        "outputId": "203817f0-b0f5-4384-8f13-ebacda110365"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model                                                     Prediction\n",
            "--------------------------------------------------------  ------------\n",
            "SVM_rbf_custom_embedding_tf_idf_without_features          Not Toxic\n",
            "SVM_rbf_custom_embedding_tf_idf_with_features             Not Toxic\n",
            "SVM_rbf_custom_embedding_tf_idf_with_features_normalised  Not Toxic\n",
            "linear_svm_avg_custom_word2vec_embedding                  Toxic\n",
            "linear_svm_avg_tfidf_custom_word2vec_embedding            Toxic\n",
            "svm_rbf_avg_custom_word2vec_embedding                     Toxic\n",
            "sgd_classifier_avg_custom_word2vec_embedding              Toxic\n",
            "sgd_classifier_avg_tfidf_custom_word2vec_embedding        Toxic\n",
            "svm_rbf_doc2vec_custom_embedding                          Not Toxic\n",
            "sgd_doc2vec_custom_embedding                              Not Toxic\n",
            "linear_svm_countVectorizer_tfdif                          Toxic\n",
            "linear_svm_countVectorizer_tfdif_nystroem_approximation   Toxic\n",
            "linear_svm_countVectorizer_tfdif_fourier_approximation    Toxic\n",
            "svm_rbf_countVectorizer_tfdif                             Toxic\n",
            "sgd_svm_countVectorizer_tfdif                             Toxic\n",
            "sgd_svm_countVectorizer_tfdif_nystroem_approximation      Toxic\n",
            "sgd_svm_countVectorizer_tfdif_fourier_approximation       Toxic\n",
            "model1_flatten_svm                                        Not Toxic\n",
            "model1_oversampled_flatten_svm                            Not Toxic\n",
            "model3_flatten_svm                                        Not Toxic\n",
            "model3_conv1d_svm                                         Not Toxic\n",
            "sgd_classifier_with_NN_preprocessing                      Not Toxic\n",
            "SVM_rbf_sentence_embedding                                Toxic\n",
            "SVM_sigmoid_sentence_embedding                            Not Toxic\n",
            "SVM_polynomial_sentence_embedding                         Toxic\n",
            "Model                                                     Prediction\n",
            "--------------------------------------------------------  ------------\n",
            "SVM_rbf_custom_embedding_tf_idf_without_features          Toxic\n",
            "SVM_rbf_custom_embedding_tf_idf_with_features             Not Toxic\n",
            "SVM_rbf_custom_embedding_tf_idf_with_features_normalised  Not Toxic\n",
            "linear_svm_avg_custom_word2vec_embedding                  Toxic\n",
            "linear_svm_avg_tfidf_custom_word2vec_embedding            Toxic\n",
            "svm_rbf_avg_custom_word2vec_embedding                     Toxic\n",
            "sgd_classifier_avg_custom_word2vec_embedding              Toxic\n",
            "sgd_classifier_avg_tfidf_custom_word2vec_embedding        Toxic\n",
            "svm_rbf_doc2vec_custom_embedding                          Toxic\n",
            "sgd_doc2vec_custom_embedding                              Toxic\n",
            "linear_svm_countVectorizer_tfdif                          Toxic\n",
            "linear_svm_countVectorizer_tfdif_nystroem_approximation   Not Toxic\n",
            "linear_svm_countVectorizer_tfdif_fourier_approximation    Toxic\n",
            "svm_rbf_countVectorizer_tfdif                             Toxic\n",
            "sgd_svm_countVectorizer_tfdif                             Toxic\n",
            "sgd_svm_countVectorizer_tfdif_nystroem_approximation      Not Toxic\n",
            "sgd_svm_countVectorizer_tfdif_fourier_approximation       Toxic\n",
            "model1_flatten_svm                                        Not Toxic\n",
            "model1_oversampled_flatten_svm                            Not Toxic\n",
            "model3_flatten_svm                                        Toxic\n",
            "model3_conv1d_svm                                         Toxic\n",
            "sgd_classifier_with_NN_preprocessing                      Not Toxic\n",
            "SVM_rbf_sentence_embedding                                Toxic\n",
            "SVM_sigmoid_sentence_embedding                            Not Toxic\n",
            "SVM_polynomial_sentence_embedding                         Toxic\n"
          ]
        }
      ],
      "source": [
        "#Longer sentences given more weightage.\n",
        "Predict(\"You bastard.\")\n",
        "Predict(\"You bastard. You cann't do anything properly. Sucha useless mess\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 96,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6aOe3qy7tSa3",
        "outputId": "78887acf-3e32-4877-b1e3-f3b641d59671"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model                                                     Prediction\n",
            "--------------------------------------------------------  ------------\n",
            "SVM_rbf_custom_embedding_tf_idf_without_features          Toxic\n",
            "SVM_rbf_custom_embedding_tf_idf_with_features             Toxic\n",
            "SVM_rbf_custom_embedding_tf_idf_with_features_normalised  Toxic\n",
            "linear_svm_avg_custom_word2vec_embedding                  Toxic\n",
            "linear_svm_avg_tfidf_custom_word2vec_embedding            Toxic\n",
            "svm_rbf_avg_custom_word2vec_embedding                     Toxic\n",
            "sgd_classifier_avg_custom_word2vec_embedding              Toxic\n",
            "sgd_classifier_avg_tfidf_custom_word2vec_embedding        Toxic\n",
            "svm_rbf_doc2vec_custom_embedding                          Toxic\n",
            "sgd_doc2vec_custom_embedding                              Toxic\n",
            "linear_svm_countVectorizer_tfdif                          Toxic\n",
            "linear_svm_countVectorizer_tfdif_nystroem_approximation   Not Toxic\n",
            "linear_svm_countVectorizer_tfdif_fourier_approximation    Not Toxic\n",
            "svm_rbf_countVectorizer_tfdif                             Toxic\n",
            "sgd_svm_countVectorizer_tfdif                             Toxic\n",
            "sgd_svm_countVectorizer_tfdif_nystroem_approximation      Not Toxic\n",
            "sgd_svm_countVectorizer_tfdif_fourier_approximation       Not Toxic\n",
            "model1_flatten_svm                                        Not Toxic\n",
            "model1_oversampled_flatten_svm                            Not Toxic\n",
            "model3_flatten_svm                                        Not Toxic\n",
            "model3_conv1d_svm                                         Not Toxic\n",
            "sgd_classifier_with_NN_preprocessing                      Not Toxic\n",
            "SVM_rbf_sentence_embedding                                Not Toxic\n",
            "SVM_sigmoid_sentence_embedding                            Not Toxic\n",
            "SVM_polynomial_sentence_embedding                         Not Toxic\n",
            "\n",
            "Total Toxic Predictions =  13\n",
            "Model                                                     Prediction\n",
            "--------------------------------------------------------  ------------\n",
            "SVM_rbf_custom_embedding_tf_idf_without_features          Not Toxic\n",
            "SVM_rbf_custom_embedding_tf_idf_with_features             Not Toxic\n",
            "SVM_rbf_custom_embedding_tf_idf_with_features_normalised  Not Toxic\n",
            "linear_svm_avg_custom_word2vec_embedding                  Not Toxic\n",
            "linear_svm_avg_tfidf_custom_word2vec_embedding            Not Toxic\n",
            "svm_rbf_avg_custom_word2vec_embedding                     Not Toxic\n",
            "sgd_classifier_avg_custom_word2vec_embedding              Not Toxic\n",
            "sgd_classifier_avg_tfidf_custom_word2vec_embedding        Toxic\n",
            "svm_rbf_doc2vec_custom_embedding                          Not Toxic\n",
            "sgd_doc2vec_custom_embedding                              Not Toxic\n",
            "linear_svm_countVectorizer_tfdif                          Not Toxic\n",
            "linear_svm_countVectorizer_tfdif_nystroem_approximation   Not Toxic\n",
            "linear_svm_countVectorizer_tfdif_fourier_approximation    Toxic\n",
            "svm_rbf_countVectorizer_tfdif                             Not Toxic\n",
            "sgd_svm_countVectorizer_tfdif                             Not Toxic\n",
            "sgd_svm_countVectorizer_tfdif_nystroem_approximation      Not Toxic\n",
            "sgd_svm_countVectorizer_tfdif_fourier_approximation       Not Toxic\n",
            "model1_flatten_svm                                        Not Toxic\n",
            "model1_oversampled_flatten_svm                            Not Toxic\n",
            "model3_flatten_svm                                        Not Toxic\n",
            "model3_conv1d_svm                                         Not Toxic\n",
            "sgd_classifier_with_NN_preprocessing                      Not Toxic\n",
            "SVM_rbf_sentence_embedding                                Not Toxic\n",
            "SVM_sigmoid_sentence_embedding                            Not Toxic\n",
            "SVM_polynomial_sentence_embedding                         Not Toxic\n",
            "\n",
            "Total Toxic Predictions =  2\n"
          ]
        }
      ],
      "source": [
        "#Some words that appears with slangs but are not individually obscene or toxic are classified as Toxic.\n",
        "Predict(\"Your MOTHER is good.\")\n",
        "Predict(\"Your SISTER is good.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 86,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BSdJhkRoFPD7",
        "outputId": "70d486e4-31a8-4cf7-cbcb-781c6184fc7b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model                                                     Prediction\n",
            "--------------------------------------------------------  ------------\n",
            "SVM_rbf_custom_embedding_tf_idf_without_features          Toxic\n",
            "SVM_rbf_custom_embedding_tf_idf_with_features             Not Toxic\n",
            "SVM_rbf_custom_embedding_tf_idf_with_features_normalised  Not Toxic\n",
            "linear_svm_avg_custom_word2vec_embedding                  Toxic\n",
            "linear_svm_avg_tfidf_custom_word2vec_embedding            Toxic\n",
            "svm_rbf_avg_custom_word2vec_embedding                     Not Toxic\n",
            "sgd_classifier_avg_custom_word2vec_embedding              Toxic\n",
            "sgd_classifier_avg_tfidf_custom_word2vec_embedding        Toxic\n",
            "svm_rbf_doc2vec_custom_embedding                          Not Toxic\n",
            "sgd_doc2vec_custom_embedding                              Not Toxic\n",
            "linear_svm_countVectorizer_tfdif                          Not Toxic\n",
            "linear_svm_countVectorizer_tfdif_nystroem_approximation   Toxic\n",
            "linear_svm_countVectorizer_tfdif_fourier_approximation    Toxic\n",
            "svm_rbf_countVectorizer_tfdif                             Not Toxic\n",
            "sgd_svm_countVectorizer_tfdif                             Toxic\n",
            "sgd_svm_countVectorizer_tfdif_nystroem_approximation      Toxic\n",
            "sgd_svm_countVectorizer_tfdif_fourier_approximation       Toxic\n",
            "model1_flatten_svm                                        Not Toxic\n",
            "model1_oversampled_flatten_svm                            Not Toxic\n",
            "model3_flatten_svm                                        Not Toxic\n",
            "model3_conv1d_svm                                         Not Toxic\n",
            "sgd_classifier_with_NN_preprocessing                      Not Toxic\n",
            "SVM_rbf_sentence_embedding                                Not Toxic\n",
            "SVM_sigmoid_sentence_embedding                            Not Toxic\n",
            "SVM_polynomial_sentence_embedding                         Not Toxic\n",
            "Model                                                     Prediction\n",
            "--------------------------------------------------------  ------------\n",
            "SVM_rbf_custom_embedding_tf_idf_without_features          Toxic\n",
            "SVM_rbf_custom_embedding_tf_idf_with_features             Not Toxic\n",
            "SVM_rbf_custom_embedding_tf_idf_with_features_normalised  Not Toxic\n",
            "linear_svm_avg_custom_word2vec_embedding                  Toxic\n",
            "linear_svm_avg_tfidf_custom_word2vec_embedding            Toxic\n",
            "svm_rbf_avg_custom_word2vec_embedding                     Not Toxic\n",
            "sgd_classifier_avg_custom_word2vec_embedding              Toxic\n",
            "sgd_classifier_avg_tfidf_custom_word2vec_embedding        Toxic\n",
            "svm_rbf_doc2vec_custom_embedding                          Not Toxic\n",
            "sgd_doc2vec_custom_embedding                              Not Toxic\n",
            "linear_svm_countVectorizer_tfdif                          Not Toxic\n",
            "linear_svm_countVectorizer_tfdif_nystroem_approximation   Not Toxic\n",
            "linear_svm_countVectorizer_tfdif_fourier_approximation    Not Toxic\n",
            "svm_rbf_countVectorizer_tfdif                             Not Toxic\n",
            "sgd_svm_countVectorizer_tfdif                             Not Toxic\n",
            "sgd_svm_countVectorizer_tfdif_nystroem_approximation      Not Toxic\n",
            "sgd_svm_countVectorizer_tfdif_fourier_approximation       Not Toxic\n",
            "model1_flatten_svm                                        Not Toxic\n",
            "model1_oversampled_flatten_svm                            Not Toxic\n",
            "model3_flatten_svm                                        Not Toxic\n",
            "model3_conv1d_svm                                         Not Toxic\n",
            "sgd_classifier_with_NN_preprocessing                      Not Toxic\n",
            "SVM_rbf_sentence_embedding                                Not Toxic\n",
            "SVM_sigmoid_sentence_embedding                            Not Toxic\n",
            "SVM_polynomial_sentence_embedding                         Not Toxic\n",
            "Model                                                     Prediction\n",
            "--------------------------------------------------------  ------------\n",
            "SVM_rbf_custom_embedding_tf_idf_without_features          Toxic\n",
            "SVM_rbf_custom_embedding_tf_idf_with_features             Toxic\n",
            "SVM_rbf_custom_embedding_tf_idf_with_features_normalised  Toxic\n",
            "linear_svm_avg_custom_word2vec_embedding                  Toxic\n",
            "linear_svm_avg_tfidf_custom_word2vec_embedding            Toxic\n",
            "svm_rbf_avg_custom_word2vec_embedding                     Toxic\n",
            "sgd_classifier_avg_custom_word2vec_embedding              Toxic\n",
            "sgd_classifier_avg_tfidf_custom_word2vec_embedding        Toxic\n",
            "svm_rbf_doc2vec_custom_embedding                          Not Toxic\n",
            "sgd_doc2vec_custom_embedding                              Not Toxic\n",
            "linear_svm_countVectorizer_tfdif                          Toxic\n",
            "linear_svm_countVectorizer_tfdif_nystroem_approximation   Not Toxic\n",
            "linear_svm_countVectorizer_tfdif_fourier_approximation    Not Toxic\n",
            "svm_rbf_countVectorizer_tfdif                             Toxic\n",
            "sgd_svm_countVectorizer_tfdif                             Toxic\n",
            "sgd_svm_countVectorizer_tfdif_nystroem_approximation      Not Toxic\n",
            "sgd_svm_countVectorizer_tfdif_fourier_approximation       Not Toxic\n",
            "model1_flatten_svm                                        Not Toxic\n",
            "model1_oversampled_flatten_svm                            Not Toxic\n",
            "model3_flatten_svm                                        Not Toxic\n",
            "model3_conv1d_svm                                         Not Toxic\n",
            "sgd_classifier_with_NN_preprocessing                      Not Toxic\n",
            "SVM_rbf_sentence_embedding                                Not Toxic\n",
            "SVM_sigmoid_sentence_embedding                            Not Toxic\n",
            "SVM_polynomial_sentence_embedding                         Not Toxic\n"
          ]
        }
      ],
      "source": [
        "Predict(\"You are of black race.\")\n",
        "Predict(\"You are of white race.\")\n",
        "Predict(\"You are of white trash race.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 87,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TsYNXeKJGqov",
        "outputId": "6467032e-e7eb-4875-8397-094ccb2a68a2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model                                                     Prediction\n",
            "--------------------------------------------------------  ------------\n",
            "SVM_rbf_custom_embedding_tf_idf_without_features          Toxic\n",
            "SVM_rbf_custom_embedding_tf_idf_with_features             Not Toxic\n",
            "SVM_rbf_custom_embedding_tf_idf_with_features_normalised  Toxic\n",
            "linear_svm_avg_custom_word2vec_embedding                  Toxic\n",
            "linear_svm_avg_tfidf_custom_word2vec_embedding            Toxic\n",
            "svm_rbf_avg_custom_word2vec_embedding                     Toxic\n",
            "sgd_classifier_avg_custom_word2vec_embedding              Toxic\n",
            "sgd_classifier_avg_tfidf_custom_word2vec_embedding        Toxic\n",
            "svm_rbf_doc2vec_custom_embedding                          Not Toxic\n",
            "sgd_doc2vec_custom_embedding                              Not Toxic\n",
            "linear_svm_countVectorizer_tfdif                          Toxic\n",
            "linear_svm_countVectorizer_tfdif_nystroem_approximation   Toxic\n",
            "linear_svm_countVectorizer_tfdif_fourier_approximation    Toxic\n",
            "svm_rbf_countVectorizer_tfdif                             Toxic\n",
            "sgd_svm_countVectorizer_tfdif                             Toxic\n",
            "sgd_svm_countVectorizer_tfdif_nystroem_approximation      Toxic\n",
            "sgd_svm_countVectorizer_tfdif_fourier_approximation       Toxic\n",
            "model1_flatten_svm                                        Not Toxic\n",
            "model1_oversampled_flatten_svm                            Not Toxic\n",
            "model3_flatten_svm                                        Not Toxic\n",
            "model3_conv1d_svm                                         Toxic\n",
            "sgd_classifier_with_NN_preprocessing                      Not Toxic\n",
            "SVM_rbf_sentence_embedding                                Not Toxic\n",
            "SVM_sigmoid_sentence_embedding                            Toxic\n",
            "SVM_polynomial_sentence_embedding                         Not Toxic\n",
            "Model                                                     Prediction\n",
            "--------------------------------------------------------  ------------\n",
            "SVM_rbf_custom_embedding_tf_idf_without_features          Not Toxic\n",
            "SVM_rbf_custom_embedding_tf_idf_with_features             Not Toxic\n",
            "SVM_rbf_custom_embedding_tf_idf_with_features_normalised  Not Toxic\n",
            "linear_svm_avg_custom_word2vec_embedding                  Toxic\n",
            "linear_svm_avg_tfidf_custom_word2vec_embedding            Toxic\n",
            "svm_rbf_avg_custom_word2vec_embedding                     Not Toxic\n",
            "sgd_classifier_avg_custom_word2vec_embedding              Toxic\n",
            "sgd_classifier_avg_tfidf_custom_word2vec_embedding        Toxic\n",
            "svm_rbf_doc2vec_custom_embedding                          Not Toxic\n",
            "sgd_doc2vec_custom_embedding                              Not Toxic\n",
            "linear_svm_countVectorizer_tfdif                          Not Toxic\n",
            "linear_svm_countVectorizer_tfdif_nystroem_approximation   Not Toxic\n",
            "linear_svm_countVectorizer_tfdif_fourier_approximation    Not Toxic\n",
            "svm_rbf_countVectorizer_tfdif                             Not Toxic\n",
            "sgd_svm_countVectorizer_tfdif                             Not Toxic\n",
            "sgd_svm_countVectorizer_tfdif_nystroem_approximation      Not Toxic\n",
            "sgd_svm_countVectorizer_tfdif_fourier_approximation       Not Toxic\n",
            "model1_flatten_svm                                        Not Toxic\n",
            "model1_oversampled_flatten_svm                            Not Toxic\n",
            "model3_flatten_svm                                        Not Toxic\n",
            "model3_conv1d_svm                                         Not Toxic\n",
            "sgd_classifier_with_NN_preprocessing                      Not Toxic\n",
            "SVM_rbf_sentence_embedding                                Not Toxic\n",
            "SVM_sigmoid_sentence_embedding                            Toxic\n",
            "SVM_polynomial_sentence_embedding                         Not Toxic\n"
          ]
        }
      ],
      "source": [
        "Predict(\"You are a smart ass.\")\n",
        "Predict(\"You are smart.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 88,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lp7lW0NcIYpn",
        "outputId": "80efb940-39a1-499e-8168-c11e20453b57"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model                                                     Prediction\n",
            "--------------------------------------------------------  ------------\n",
            "SVM_rbf_custom_embedding_tf_idf_without_features          Not Toxic\n",
            "SVM_rbf_custom_embedding_tf_idf_with_features             Not Toxic\n",
            "SVM_rbf_custom_embedding_tf_idf_with_features_normalised  Not Toxic\n",
            "linear_svm_avg_custom_word2vec_embedding                  Toxic\n",
            "linear_svm_avg_tfidf_custom_word2vec_embedding            Toxic\n",
            "svm_rbf_avg_custom_word2vec_embedding                     Toxic\n",
            "sgd_classifier_avg_custom_word2vec_embedding              Toxic\n",
            "sgd_classifier_avg_tfidf_custom_word2vec_embedding        Toxic\n",
            "svm_rbf_doc2vec_custom_embedding                          Not Toxic\n",
            "sgd_doc2vec_custom_embedding                              Not Toxic\n",
            "linear_svm_countVectorizer_tfdif                          Toxic\n",
            "linear_svm_countVectorizer_tfdif_nystroem_approximation   Toxic\n",
            "linear_svm_countVectorizer_tfdif_fourier_approximation    Not Toxic\n",
            "svm_rbf_countVectorizer_tfdif                             Toxic\n",
            "sgd_svm_countVectorizer_tfdif                             Toxic\n",
            "sgd_svm_countVectorizer_tfdif_nystroem_approximation      Toxic\n",
            "sgd_svm_countVectorizer_tfdif_fourier_approximation       Not Toxic\n",
            "model1_flatten_svm                                        Not Toxic\n",
            "model1_oversampled_flatten_svm                            Not Toxic\n",
            "model3_flatten_svm                                        Toxic\n",
            "model3_conv1d_svm                                         Not Toxic\n",
            "sgd_classifier_with_NN_preprocessing                      Not Toxic\n",
            "SVM_rbf_sentence_embedding                                Not Toxic\n",
            "SVM_sigmoid_sentence_embedding                            Not Toxic\n",
            "SVM_polynomial_sentence_embedding                         Not Toxic\n",
            "Model                                                     Prediction\n",
            "--------------------------------------------------------  ------------\n",
            "SVM_rbf_custom_embedding_tf_idf_without_features          Not Toxic\n",
            "SVM_rbf_custom_embedding_tf_idf_with_features             Not Toxic\n",
            "SVM_rbf_custom_embedding_tf_idf_with_features_normalised  Not Toxic\n",
            "linear_svm_avg_custom_word2vec_embedding                  Toxic\n",
            "linear_svm_avg_tfidf_custom_word2vec_embedding            Toxic\n",
            "svm_rbf_avg_custom_word2vec_embedding                     Toxic\n",
            "sgd_classifier_avg_custom_word2vec_embedding              Toxic\n",
            "sgd_classifier_avg_tfidf_custom_word2vec_embedding        Toxic\n",
            "svm_rbf_doc2vec_custom_embedding                          Not Toxic\n",
            "sgd_doc2vec_custom_embedding                              Not Toxic\n",
            "linear_svm_countVectorizer_tfdif                          Toxic\n",
            "linear_svm_countVectorizer_tfdif_nystroem_approximation   Toxic\n",
            "linear_svm_countVectorizer_tfdif_fourier_approximation    Not Toxic\n",
            "svm_rbf_countVectorizer_tfdif                             Toxic\n",
            "sgd_svm_countVectorizer_tfdif                             Toxic\n",
            "sgd_svm_countVectorizer_tfdif_nystroem_approximation      Toxic\n",
            "sgd_svm_countVectorizer_tfdif_fourier_approximation       Not Toxic\n",
            "model1_flatten_svm                                        Not Toxic\n",
            "model1_oversampled_flatten_svm                            Not Toxic\n",
            "model3_flatten_svm                                        Toxic\n",
            "model3_conv1d_svm                                         Toxic\n",
            "sgd_classifier_with_NN_preprocessing                      Not Toxic\n",
            "SVM_rbf_sentence_embedding                                Toxic\n",
            "SVM_sigmoid_sentence_embedding                            Not Toxic\n",
            "SVM_polynomial_sentence_embedding                         Toxic\n"
          ]
        }
      ],
      "source": [
        "Predict(\"Donald Trump is a not an asshole.\")\n",
        "Predict(\"Donald Trump is an asshole.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 89,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VmIUDaUZxzV-",
        "outputId": "df4e40a8-64d2-4a65-fb4d-60ae9af23785"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model                                                     Prediction\n",
            "--------------------------------------------------------  ------------\n",
            "SVM_rbf_custom_embedding_tf_idf_without_features          Toxic\n",
            "SVM_rbf_custom_embedding_tf_idf_with_features             Toxic\n",
            "SVM_rbf_custom_embedding_tf_idf_with_features_normalised  Toxic\n",
            "linear_svm_avg_custom_word2vec_embedding                  Toxic\n",
            "linear_svm_avg_tfidf_custom_word2vec_embedding            Toxic\n",
            "svm_rbf_avg_custom_word2vec_embedding                     Toxic\n",
            "sgd_classifier_avg_custom_word2vec_embedding              Toxic\n",
            "sgd_classifier_avg_tfidf_custom_word2vec_embedding        Toxic\n",
            "svm_rbf_doc2vec_custom_embedding                          Toxic\n",
            "sgd_doc2vec_custom_embedding                              Toxic\n",
            "linear_svm_countVectorizer_tfdif                          Toxic\n",
            "linear_svm_countVectorizer_tfdif_nystroem_approximation   Toxic\n",
            "linear_svm_countVectorizer_tfdif_fourier_approximation    Toxic\n",
            "svm_rbf_countVectorizer_tfdif                             Toxic\n",
            "sgd_svm_countVectorizer_tfdif                             Toxic\n",
            "sgd_svm_countVectorizer_tfdif_nystroem_approximation      Toxic\n",
            "sgd_svm_countVectorizer_tfdif_fourier_approximation       Toxic\n",
            "model1_flatten_svm                                        Not Toxic\n",
            "model1_oversampled_flatten_svm                            Not Toxic\n",
            "model3_flatten_svm                                        Toxic\n",
            "model3_conv1d_svm                                         Toxic\n",
            "sgd_classifier_with_NN_preprocessing                      Not Toxic\n",
            "SVM_rbf_sentence_embedding                                Toxic\n",
            "SVM_sigmoid_sentence_embedding                            Toxic\n",
            "SVM_polynomial_sentence_embedding                         Toxic\n"
          ]
        }
      ],
      "source": [
        "Predict(\"You mother fucking asshole peice of shit dickhead Faggot !!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 97,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3r7h5JFRyzG7",
        "outputId": "4a5e433d-1cf5-4c1c-934a-4323436cef5f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model                                                     Prediction\n",
            "--------------------------------------------------------  ------------\n",
            "SVM_rbf_custom_embedding_tf_idf_without_features          Not Toxic\n",
            "SVM_rbf_custom_embedding_tf_idf_with_features             Not Toxic\n",
            "SVM_rbf_custom_embedding_tf_idf_with_features_normalised  Not Toxic\n",
            "linear_svm_avg_custom_word2vec_embedding                  Not Toxic\n",
            "linear_svm_avg_tfidf_custom_word2vec_embedding            Not Toxic\n",
            "svm_rbf_avg_custom_word2vec_embedding                     Not Toxic\n",
            "sgd_classifier_avg_custom_word2vec_embedding              Not Toxic\n",
            "sgd_classifier_avg_tfidf_custom_word2vec_embedding        Not Toxic\n",
            "svm_rbf_doc2vec_custom_embedding                          Not Toxic\n",
            "sgd_doc2vec_custom_embedding                              Not Toxic\n",
            "linear_svm_countVectorizer_tfdif                          Not Toxic\n",
            "linear_svm_countVectorizer_tfdif_nystroem_approximation   Not Toxic\n",
            "linear_svm_countVectorizer_tfdif_fourier_approximation    Not Toxic\n",
            "svm_rbf_countVectorizer_tfdif                             Not Toxic\n",
            "sgd_svm_countVectorizer_tfdif                             Not Toxic\n",
            "sgd_svm_countVectorizer_tfdif_nystroem_approximation      Not Toxic\n",
            "sgd_svm_countVectorizer_tfdif_fourier_approximation       Not Toxic\n",
            "model1_flatten_svm                                        Not Toxic\n",
            "model1_oversampled_flatten_svm                            Not Toxic\n",
            "model3_flatten_svm                                        Not Toxic\n",
            "model3_conv1d_svm                                         Not Toxic\n",
            "sgd_classifier_with_NN_preprocessing                      Not Toxic\n",
            "sgd_classifier_oversample_with_NN_preprocessing           Toxic\n",
            "SVM_rbf_sentence_embedding                                Not Toxic\n",
            "SVM_sigmoid_sentence_embedding                            Not Toxic\n",
            "SVM_polynomial_sentence_embedding                         Not Toxic\n",
            "\n",
            "Total Toxic Predictions =  1\n"
          ]
        }
      ],
      "source": [
        "Predict(\"I love this airlines for sending me to london and my luggage to Delhi.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 100,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5Hs6ErOy2pFS",
        "outputId": "6a2444e3-ec02-4838-9b51-3f28ca11e423"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model                                                     Prediction\n",
            "--------------------------------------------------------  ------------\n",
            "SVM_rbf_custom_embedding_tf_idf_without_features          Not Toxic\n",
            "SVM_rbf_custom_embedding_tf_idf_with_features             Not Toxic\n",
            "SVM_rbf_custom_embedding_tf_idf_with_features_normalised  Not Toxic\n",
            "linear_svm_avg_custom_word2vec_embedding                  Not Toxic\n",
            "linear_svm_avg_tfidf_custom_word2vec_embedding            Not Toxic\n",
            "svm_rbf_avg_custom_word2vec_embedding                     Not Toxic\n",
            "sgd_classifier_avg_custom_word2vec_embedding              Not Toxic\n",
            "sgd_classifier_avg_tfidf_custom_word2vec_embedding        Toxic\n",
            "svm_rbf_doc2vec_custom_embedding                          Not Toxic\n",
            "sgd_doc2vec_custom_embedding                              Not Toxic\n",
            "linear_svm_countVectorizer_tfdif                          Toxic\n",
            "linear_svm_countVectorizer_tfdif_nystroem_approximation   Not Toxic\n",
            "linear_svm_countVectorizer_tfdif_fourier_approximation    Not Toxic\n",
            "svm_rbf_countVectorizer_tfdif                             Toxic\n",
            "sgd_svm_countVectorizer_tfdif                             Toxic\n",
            "sgd_svm_countVectorizer_tfdif_nystroem_approximation      Not Toxic\n",
            "sgd_svm_countVectorizer_tfdif_fourier_approximation       Not Toxic\n",
            "model1_flatten_svm                                        Not Toxic\n",
            "model1_oversampled_flatten_svm                            Not Toxic\n",
            "model3_flatten_svm                                        Not Toxic\n",
            "model3_conv1d_svm                                         Not Toxic\n",
            "sgd_classifier_with_NN_preprocessing                      Not Toxic\n",
            "SVM_rbf_sentence_embedding                                Toxic\n",
            "SVM_sigmoid_sentence_embedding                            Not Toxic\n",
            "SVM_polynomial_sentence_embedding                         Toxic\n",
            "\n",
            "Total Toxic Predictions =  6\n"
          ]
        }
      ],
      "source": [
        "Predict(\"Why don't these Bastards leave us in peace?\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D0uPQcwv29oM"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "lLmApzq349rG"
      ],
      "name": "NLP-Analysis.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
